{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios 1 Tema 2 PLN (Análisis textual y recursos lingüísticos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wrst/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "\n",
    "# Importamos NLTK (Natural Language Tool-Kit)\n",
    "import nltk\n",
    "\n",
    "# Importamos algunos tokenizers\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Importamos utilidades probabilísticas\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Se pide implementar un código en Python para tokenizar un texto en inglés.  \n",
    "- **Punkt:** tokenizador de `nltk`, divide el texto en sentencias por defecto.\n",
    "- Ejemplo de uso de **punkt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto tokenizado:\n",
      "['We met Miss.', 'Tanaya Das and Mr.', 'Rohan Singh today.', 'They are pursuing a B.tech degree in Data Science.']\n"
     ]
    }
   ],
   "source": [
    "# Texto de ejemplo\n",
    "text = \"We met Miss. Tanaya Das and Mr. Rohan Singh today. They are pursuing a B.tech degree in Data Science.\"\n",
    "\n",
    "# Inicializamos el tokenizador\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Tokenizamos el texto en sentencias\n",
    "sentences = tokenizer.tokenize(text)\n",
    "print(f\"Texto tokenizado:\\n{sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teniendo un texto en el que haya saltos de línea, cada frase se mostrará tokenizada en una fila distinta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'US', 'Open', 'will', 'become', 'a', '15-day', 'tournament', 'in', '2025', ',', 'beginning', 'on', 'a', 'weekend', 'for', 'the', 'first', 'time', 'in', 'the', 'Open', 'era', '.']\n",
      "['This', 'year', \"'s\", 'main', 'draw', 'at', 'Flushing', 'Meadows', 'will', 'start', 'on', 'Sunday', ',', '24', 'August', 'and', 'end', 'on', 'Sunday', ',', '7', 'September', '.']\n",
      "['It', 'becomes', 'the', 'latest', 'Grand', 'Slam', 'to', 'announce', 'a', 'Sunday', 'start', '.']\n",
      "['The', 'Australian', 'Open', 'expanded', 'to', 'a', '15-day', 'tournament', 'in', '2024', ',', 'after', 'the', 'French', 'Open', 'took', 'that', 'decision', 'in', '2006', '.']\n",
      "['That', 'leaves', 'Wimbledon', 'as', 'the', 'only', 'remaining', 'Slam', 'event', 'to', 'retain', 'the', 'traditional', 'Monday', 'start', '.']\n",
      "['In', 'making', 'the', 'change', ',', 'the', 'US', 'Open', 'said', 'the', 'move', 'would', 'allow', '``', 'more', 'fan', 'access', 'than', 'ever', 'to', 'the', 'main', 'draw', 'following', 'three', 'consecutive', 'years', 'of', 'record-breaking', 'attendance', \"''\", '.']\n",
      "['The', 'tournament', 'estimates', 'the', 'expansion', 'will', 'allow', 'access', 'for', 'an', 'additional', '70,000', 'spectators', '.']\n",
      "['Men', \"'s\", 'and', 'women', \"'s\", 'singles', 'first-round', 'matches', 'will', 'be', 'played', 'across', 'the', 'opening', 'three', 'days', 'in', 'New', 'York', ',', 'from', 'Sunday', 'to', 'Tuesday', '.']\n",
      "['The', 'Australian', 'Open', 'took', 'the', 'decision', 'to', 'become', 'a', '15-day', 'event', 'in', 'an', 'attempt', 'to', 'reduce', 'the', 'number', 'of', 'late-night', 'finishes', 'at', 'Melbourne', 'Park', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Cargamos el archivo de texto \"texto_ej1.txt\"\n",
    "with open(\"data/texto_ej1.txt\", \"r\") as text:\n",
    "    tokenized_text = sentence_tokenizer.tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for sentence in tokenized_text:\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    word_tokenized.append(tokenized_sentence)\n",
    "\n",
    "for sentence in word_tokenized:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 2:** Hacer lo mismo que en el ejercicio anterior, pero para un texto en español. ¿Hay alguna diferencia a la hora de tokenizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'perro', 'corrió', 'rápidamente', 'por', 'el', 'parque', '.']\n",
      "['El', 'sol', 'brillaba', 'y', 'los', 'pájaros', 'cantaban', '.']\n",
      "['Los', 'niños', 'jugaban', 'y', 'reían', 'alegremente', '.']\n",
      "['El', 'aire', 'fresco', 'olía', 'a', 'flores', 'y', 'a', 'hierba', '.']\n",
      "['Fue', 'un', 'día', 'perfecto', 'para', 'estar', 'al', 'aire', 'libre', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Cargamos el archivo de texto \"texto_ej2.txt\"\n",
    "with open(\"data/texto_ej2.txt\", \"r\") as text:\n",
    "    tokenized_text = sentence_tokenizer.tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for sentence in tokenized_text:\n",
    "    tokenized_sentence = word_tokenize(sentence, language=\"spanish\")\n",
    "    word_tokenized.append(tokenized_sentence)\n",
    "\n",
    "for sentence in word_tokenized:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 3:** ¿Qué ocurre si se tokeniza utilizando `split` en lugar de `word_tokenize`? Haz pruebas sobre diferentes textos y compara la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'US', 'Open', 'will', 'become', 'a', '15-day', 'tournament', 'in', '2025,', 'beginning', 'on', 'a', 'weekend', 'for', 'the', 'first', 'time', 'in', 'the', 'Open', 'era.']\n",
      "['This', \"year's\", 'main', 'draw', 'at', 'Flushing', 'Meadows', 'will', 'start', 'on', 'Sunday,', '24', 'August', 'and', 'end', 'on', 'Sunday,', '7', 'September.']\n",
      "['It', 'becomes', 'the', 'latest', 'Grand', 'Slam', 'to', 'announce', 'a', 'Sunday', 'start.', 'The', 'Australian', 'Open', 'expanded', 'to', 'a', '15-day', 'tournament', 'in', '2024,', 'after', 'the', 'French', 'Open', 'took', 'that', 'decision', 'in', '2006.']\n",
      "['That', 'leaves', 'Wimbledon', 'as', 'the', 'only', 'remaining', 'Slam', 'event', 'to', 'retain', 'the', 'traditional', 'Monday', 'start.']\n",
      "['In', 'making', 'the', 'change,', 'the', 'US', 'Open', 'said', 'the', 'move', 'would', 'allow', '\"more', 'fan', 'access', 'than', 'ever', 'to', 'the', 'main', 'draw', 'following', 'three', 'consecutive', 'years', 'of', 'record-breaking', 'attendance\".']\n",
      "['The', 'tournament', 'estimates', 'the', 'expansion', 'will', 'allow', 'access', 'for', 'an', 'additional', '70,000', 'spectators.']\n",
      "[\"Men's\", 'and', \"women's\", 'singles', 'first-round', 'matches', 'will', 'be', 'played', 'across', 'the', 'opening', 'three', 'days', 'in', 'New', 'York,', 'from', 'Sunday', 'to', 'Tuesday.']\n",
      "['The', 'Australian', 'Open', 'took', 'the', 'decision', 'to', 'become', 'a', '15-day', 'event', 'in', 'an', 'attempt', 'to', 'reduce', 'the', 'number', 'of', 'late-night', 'finishes', 'at', 'Melbourne', 'Park.']\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1\n",
    "with open(\"data/texto_ej1.txt\", \"r\") as text:\n",
    "    tokenized_text = []\n",
    "    for line in text:\n",
    "        text_split = line.split()\n",
    "        tokenized_text.append(text_split)\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En esta prueba observamos que se incluyen las puntuaciones (\".\", \",\", etc.) dentro del token correspondiente a la palabra a la que suceden (en lugar de codificarse en un token propio) al utilizar `split`.\n",
    "- Esto se debe a que, por defecto, el método `split` separa la sentencia en palabras de acuerdo a los espacios en blanco (\" \")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'perro', 'corrió', 'rápidamente', 'por', 'el', 'parque.']\n",
      "['El', 'sol', 'brillaba', 'y', 'los', 'pájaros', 'cantaban.']\n",
      "['Los', 'niños', 'jugaban', 'y', 'reían', 'alegremente.']\n",
      "['El', 'aire', 'fresco', 'olía', 'a', 'flores', 'y', 'a', 'hierba.']\n",
      "['Fue', 'un', 'día', 'perfecto', 'para', 'estar', 'al', 'aire', 'libre.']\n"
     ]
    }
   ],
   "source": [
    "# Prueba 2\n",
    "with open(\"data/texto_ej2.txt\", \"r\") as text:\n",
    "    tokenized_text = []\n",
    "    for line in text:\n",
    "        text_split = line.split()\n",
    "        tokenized_text.append(text_split)\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observamos el mismo suceso que en la anterior prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 4:** Dado un fichero con contenido en inglés (\"Data_Science.txt\") se quieren eliminar todos los signos de puntuación que aparezcan en el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Contenido original **\n",
      "Data science is the study of the extraction of knowledge from data. It uses various techniques from many fields, including signal processing, mathematics, probability models, machine learning, computer programming, statistics, data engineering, pattern recognition and learning, visualization, uncertainty modeling, data warehousing, and high performance computing with the goal of extracting useful knowledge from the data. Data Science is not restricted to only big data, although the fact that data is scaling up makes big data an important aspect of data science.\n",
      "A practitioner of data science is called a data scientist. Data scientists solve complex data problems using various elements of mathematics, statistics and computer science, although expertise in these subjects are not required. However, a data scientist is most likely to be an expert in only one or two of these disciplines, meaning that cross disciplinary teams can be a key component of data science.\n",
      "Good data scientists are able to apply their skills to achieve a broad spectrum of end results. The skill-sets and competencies that data scientists employ vary widely.\n",
      "\n",
      "** Contenido tokenizado sin puntuaciones **\n",
      "Data science is the study of the extraction of knowledge from data It uses various techniques from many fields including signal processing mathematics probability models machine learning computer programming statistics data engineering pattern recognition and learning visualization uncertainty modeling data warehousing and high performance computing with the goal of extracting useful knowledge from the data Data Science is not restricted to only big data although the fact that data is scaling up makes big data an important aspect of data science\n",
      "A practitioner of data science is called a data scientist Data scientists solve complex data problems using various elements of mathematics statistics and computer science although expertise in these subjects are not required However a data scientist is most likely to be an expert in only one or two of these disciplines meaning that cross disciplinary teams can be a key component of data science\n",
      "Good data scientists are able to apply their skills to achieve a broad spectrum of end results The skillsets and competencies that data scientists employ vary widely\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    contents = text.read()\n",
    "    # El patrón hará match con palabras, whitespaces y dígitos\n",
    "    tokenized_text = regexp_tokenize(text=contents, pattern=r'[\\w\\s\\d]+')\n",
    "\n",
    "with open(\"data/Data_Science_no_punct.txt\", \"w\") as text:\n",
    "    text.writelines(tokenized_text)\n",
    "\n",
    "with open(\"data/Data_Science_no_punct.txt\", \"r\") as text:\n",
    "    tokenized_content = text.read()\n",
    "\n",
    "print(\"** Contenido original **\")\n",
    "print(contents)\n",
    "print(\"** Contenido tokenizado sin puntuaciones **\")\n",
    "print(tokenized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 5:** Dado un fichero de texto con contenido en inglés (\"Data_Science.txt\") se quiere saber cuáles son las 5 palabras más frecuentes del texto, antes y después de eliminar los signos de puntuación. Además, se quiere saber cuántas veces aparece la palabra \"data\" en el texto.  \n",
    "Se puede consultar la API de NLTK: https://www.nltk.org/api/nltk.probability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'science', 'is', 'the', 'study', 'of', 'the', 'extraction', 'of', 'knowledge', 'from', 'data', '.'], ['It', 'uses', 'various', 'techniques', 'from', 'many', 'fields', ',', 'including', 'signal', 'processing', ',', 'mathematics', ',', 'probability', 'models', ',', 'machine', 'learning', ',', 'computer', 'programming', ',', 'statistics', ',', 'data', 'engineering', ',', 'pattern', 'recognition', 'and', 'learning', ',', 'visualization', ',', 'uncertainty', 'modeling', ',', 'data', 'warehousing', ',', 'and', 'high', 'performance', 'computing', 'with', 'the', 'goal', 'of', 'extracting', 'useful', 'knowledge', 'from', 'the', 'data', '.'], ['Data', 'Science', 'is', 'not', 'restricted', 'to', 'only', 'big', 'data', ',', 'although', 'the', 'fact', 'that', 'data', 'is', 'scaling', 'up', 'makes', 'big', 'data', 'an', 'important', 'aspect', 'of', 'data', 'science', '.'], ['A', 'practitioner', 'of', 'data', 'science', 'is', 'called', 'a', 'data', 'scientist', '.'], ['Data', 'scientists', 'solve', 'complex', 'data', 'problems', 'using', 'various', 'elements', 'of', 'mathematics', ',', 'statistics', 'and', 'computer', 'science', ',', 'although', 'expertise', 'in', 'these', 'subjects', 'are', 'not', 'required', '.'], ['However', ',', 'a', 'data', 'scientist', 'is', 'most', 'likely', 'to', 'be', 'an', 'expert', 'in', 'only', 'one', 'or', 'two', 'of', 'these', 'disciplines', ',', 'meaning', 'that', 'cross', 'disciplinary', 'teams', 'can', 'be', 'a', 'key', 'component', 'of', 'data', 'science', '.'], ['Good', 'data', 'scientists', 'are', 'able', 'to', 'apply', 'their', 'skills', 'to', 'achieve', 'a', 'broad', 'spectrum', 'of', 'end', 'results', '.'], ['The', 'skill-sets', 'and', 'competencies', 'that', 'data', 'scientists', 'employ', 'vary', 'widely', '.']]\n",
      "[['Data', 'science', 'is', 'the', 'study', 'of', 'the', 'extraction', 'of', 'knowledge', 'from', 'data', 'It', 'uses', 'various', 'techniques', 'from', 'many', 'fields', 'including', 'signal', 'processing', 'mathematics', 'probability', 'models', 'machine', 'learning', 'computer', 'programming', 'statistics', 'data', 'engineering', 'pattern', 'recognition', 'and', 'learning', 'visualization', 'uncertainty', 'modeling', 'data', 'warehousing', 'and', 'high', 'performance', 'computing', 'with', 'the', 'goal', 'of', 'extracting', 'useful', 'knowledge', 'from', 'the', 'data', 'Data', 'Science', 'is', 'not', 'restricted', 'to', 'only', 'big', 'data', 'although', 'the', 'fact', 'that', 'data', 'is', 'scaling', 'up', 'makes', 'big', 'data', 'an', 'important', 'aspect', 'of', 'data', 'science', 'A', 'practitioner', 'of', 'data', 'science', 'is', 'called', 'a', 'data', 'scientist', 'Data', 'scientists', 'solve', 'complex', 'data', 'problems', 'using', 'various', 'elements', 'of', 'mathematics', 'statistics', 'and', 'computer', 'science', 'although', 'expertise', 'in', 'these', 'subjects', 'are', 'not', 'required', 'However', 'a', 'data', 'scientist', 'is', 'most', 'likely', 'to', 'be', 'an', 'expert', 'in', 'only', 'one', 'or', 'two', 'of', 'these', 'disciplines', 'meaning', 'that', 'cross', 'disciplinary', 'teams', 'can', 'be', 'a', 'key', 'component', 'of', 'data', 'science', 'Good', 'data', 'scientists', 'are', 'able', 'to', 'apply', 'their', 'skills', 'to', 'achieve', 'a', 'broad', 'spectrum', 'of', 'end', 'results', 'The', 'skillsets', 'and', 'competencies', 'that', 'data', 'scientists', 'employ', 'vary', 'widely']]\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Leemos el fichero \"Data_Science.txt\" y lo almacenamos en una variable\n",
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    data_science = text.read()\n",
    "    sent_tokenized1 = sent_tokenizer.tokenize(data_science)\n",
    "    word_tokenized1 = []\n",
    "    for sentence in sent_tokenized1:\n",
    "        word_tokenized1.append(word_tokenize(sentence))\n",
    "\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Leemos el fichero \"Data_Science_no_punct.txt\" y lo almacenamos en una variable\n",
    "with open(\"data/Data_Science_no_punct.txt\", \"r\") as text:\n",
    "    data_science_no_punct = text.read()\n",
    "    sent_tokenized2 = sent_tokenizer.tokenize(data_science_no_punct)\n",
    "    word_tokenized2 = []\n",
    "    for sentence in sent_tokenized2:\n",
    "        word_tokenized2.append(word_tokenize(sentence))\n",
    "\n",
    "print(word_tokenized1)\n",
    "print(word_tokenized2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
