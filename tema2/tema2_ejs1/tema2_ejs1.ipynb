{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios 1 Tema 2 PLN (Análisis textual y recursos lingüísticos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/wrst/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "# Importamos NLTK (Natural Language Tool-Kit)\n",
    "import nltk\n",
    "\n",
    "# Importamos algunos tokenizers\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Importamos utilidades probabilísticas\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Importamos las stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importamos el PoS tagger\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Importamos el stemmer con el algoritmo de Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Importamos el lematizador\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"universal_tagset\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Se pide implementar un código en Python para tokenizar un texto en inglés.  \n",
    "- **Punkt:** tokenizador de `nltk`, divide el texto en sentencias por defecto.\n",
    "- Ejemplo de uso de **punkt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto tokenizado:\n",
      "['We met Miss.', 'Tanaya Das and Mr.', 'Rohan Singh today.', 'They are pursuing a B.tech degree in Data Science.']\n"
     ]
    }
   ],
   "source": [
    "# Texto de ejemplo\n",
    "text = \"We met Miss. Tanaya Das and Mr. Rohan Singh today. They are pursuing a B.tech degree in Data Science.\"\n",
    "\n",
    "# Inicializamos el tokenizador\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Tokenizamos el texto en sentencias\n",
    "sentences = tokenizer.tokenize(text)\n",
    "print(f\"Texto tokenizado:\\n{sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teniendo un texto en el que haya saltos de línea, cada frase se mostrará tokenizada en una fila distinta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'US', 'Open', 'will', 'become', 'a', '15-day', 'tournament', 'in', '2025', ',', 'beginning', 'on', 'a', 'weekend', 'for', 'the', 'first', 'time', 'in', 'the', 'Open', 'era', '.']\n",
      "['This', 'year', \"'s\", 'main', 'draw', 'at', 'Flushing', 'Meadows', 'will', 'start', 'on', 'Sunday', ',', '24', 'August', 'and', 'end', 'on', 'Sunday', ',', '7', 'September', '.']\n",
      "['It', 'becomes', 'the', 'latest', 'Grand', 'Slam', 'to', 'announce', 'a', 'Sunday', 'start', '.']\n",
      "['The', 'Australian', 'Open', 'expanded', 'to', 'a', '15-day', 'tournament', 'in', '2024', ',', 'after', 'the', 'French', 'Open', 'took', 'that', 'decision', 'in', '2006', '.']\n",
      "['That', 'leaves', 'Wimbledon', 'as', 'the', 'only', 'remaining', 'Slam', 'event', 'to', 'retain', 'the', 'traditional', 'Monday', 'start', '.']\n",
      "['In', 'making', 'the', 'change', ',', 'the', 'US', 'Open', 'said', 'the', 'move', 'would', 'allow', '``', 'more', 'fan', 'access', 'than', 'ever', 'to', 'the', 'main', 'draw', 'following', 'three', 'consecutive', 'years', 'of', 'record-breaking', 'attendance', \"''\", '.']\n",
      "['The', 'tournament', 'estimates', 'the', 'expansion', 'will', 'allow', 'access', 'for', 'an', 'additional', '70,000', 'spectators', '.']\n",
      "['Men', \"'s\", 'and', 'women', \"'s\", 'singles', 'first-round', 'matches', 'will', 'be', 'played', 'across', 'the', 'opening', 'three', 'days', 'in', 'New', 'York', ',', 'from', 'Sunday', 'to', 'Tuesday', '.']\n",
      "['The', 'Australian', 'Open', 'took', 'the', 'decision', 'to', 'become', 'a', '15-day', 'event', 'in', 'an', 'attempt', 'to', 'reduce', 'the', 'number', 'of', 'late-night', 'finishes', 'at', 'Melbourne', 'Park', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo de texto \"texto_ej1.txt\"\n",
    "with open(\"data/texto_ej1.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized_sent = word_tokenize(s)\n",
    "    word_tokenized.append(word_tokenized_sent)\n",
    "\n",
    "for s in word_tokenized:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 2:** Hacer lo mismo que en el ejercicio anterior, pero para un texto en español. ¿Hay alguna diferencia a la hora de tokenizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'perro', 'corrió', 'rápidamente', 'por', 'el', 'parque', '.']\n",
      "['El', 'sol', 'brillaba', 'y', 'los', 'pájaros', 'cantaban', '.']\n",
      "['Los', 'niños', 'jugaban', 'y', 'reían', 'alegremente', '.']\n",
      "['El', 'aire', 'fresco', 'olía', 'a', 'flores', 'y', 'a', 'hierba', '.']\n",
      "['Fue', 'un', 'día', 'perfecto', 'para', 'estar', 'al', 'aire', 'libre', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo de texto \"texto_ej2.txt\"\n",
    "with open(\"data/texto_ej2.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized_sent = word_tokenize(s, language=\"spanish\")\n",
    "    word_tokenized.append(word_tokenized_sent)\n",
    "\n",
    "for s in word_tokenized:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 3:** ¿Qué ocurre si se tokeniza utilizando `split` en lugar de `word_tokenize`? Haz pruebas sobre diferentes textos y compara la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'US', 'Open', 'will', 'become', 'a', '15-day', 'tournament', 'in', '2025,', 'beginning', 'on', 'a', 'weekend', 'for', 'the', 'first', 'time', 'in', 'the', 'Open', 'era.']\n",
      "['This', \"year's\", 'main', 'draw', 'at', 'Flushing', 'Meadows', 'will', 'start', 'on', 'Sunday,', '24', 'August', 'and', 'end', 'on', 'Sunday,', '7', 'September.']\n",
      "['It', 'becomes', 'the', 'latest', 'Grand', 'Slam', 'to', 'announce', 'a', 'Sunday', 'start.', 'The', 'Australian', 'Open', 'expanded', 'to', 'a', '15-day', 'tournament', 'in', '2024,', 'after', 'the', 'French', 'Open', 'took', 'that', 'decision', 'in', '2006.']\n",
      "['That', 'leaves', 'Wimbledon', 'as', 'the', 'only', 'remaining', 'Slam', 'event', 'to', 'retain', 'the', 'traditional', 'Monday', 'start.']\n",
      "['In', 'making', 'the', 'change,', 'the', 'US', 'Open', 'said', 'the', 'move', 'would', 'allow', '\"more', 'fan', 'access', 'than', 'ever', 'to', 'the', 'main', 'draw', 'following', 'three', 'consecutive', 'years', 'of', 'record-breaking', 'attendance\".']\n",
      "['The', 'tournament', 'estimates', 'the', 'expansion', 'will', 'allow', 'access', 'for', 'an', 'additional', '70,000', 'spectators.']\n",
      "[\"Men's\", 'and', \"women's\", 'singles', 'first-round', 'matches', 'will', 'be', 'played', 'across', 'the', 'opening', 'three', 'days', 'in', 'New', 'York,', 'from', 'Sunday', 'to', 'Tuesday.']\n",
      "['The', 'Australian', 'Open', 'took', 'the', 'decision', 'to', 'become', 'a', '15-day', 'event', 'in', 'an', 'attempt', 'to', 'reduce', 'the', 'number', 'of', 'late-night', 'finishes', 'at', 'Melbourne', 'Park.']\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1\n",
    "with open(\"data/texto_ej1.txt\", \"r\") as text:\n",
    "    word_tokenized = []\n",
    "    for s in text:\n",
    "        word_tokenized_sent = s.split()\n",
    "        word_tokenized.append(word_tokenized_sent)\n",
    "\n",
    "for sentence in word_tokenized:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En esta prueba observamos que se incluyen las puntuaciones (\".\", \",\", etc.) dentro del token correspondiente a la palabra a la que suceden (en lugar de codificarse en un token propio) al utilizar `split`.\n",
    "- Esto se debe a que, por defecto, el método `split` separa la sentencia en palabras de acuerdo a los espacios en blanco (\" \")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'perro', 'corrió', 'rápidamente', 'por', 'el', 'parque.']\n",
      "['El', 'sol', 'brillaba', 'y', 'los', 'pájaros', 'cantaban.']\n",
      "['Los', 'niños', 'jugaban', 'y', 'reían', 'alegremente.']\n",
      "['El', 'aire', 'fresco', 'olía', 'a', 'flores', 'y', 'a', 'hierba.']\n",
      "['Fue', 'un', 'día', 'perfecto', 'para', 'estar', 'al', 'aire', 'libre.']\n"
     ]
    }
   ],
   "source": [
    "# Prueba 2\n",
    "with open(\"data/texto_ej2.txt\", \"r\") as text:\n",
    "    word_tokenized = []\n",
    "    for s in text:\n",
    "        word_tokenized_sent = s.split()\n",
    "        word_tokenized.append(word_tokenized_sent)\n",
    "\n",
    "for s in word_tokenized:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observamos el mismo suceso que en la anterior prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 4:** Dado un fichero con contenido en inglés (\"Data_Science.txt\") se quieren eliminar todos los signos de puntuación que aparezcan en el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'science', 'is', 'the', 'study', 'of', 'the', 'extraction', 'of', 'knowledge', 'from', 'data'], ['It', 'uses', 'various', 'techniques', 'from', 'many', 'fields', 'including', 'signal', 'processing', 'mathematics', 'probability', 'models', 'machine', 'learning', 'computer', 'programming', 'statistics', 'data', 'engineering', 'pattern', 'recognition', 'and', 'learning', 'visualization', 'uncertainty', 'modeling', 'data', 'warehousing', 'and', 'high', 'performance', 'computing', 'with', 'the', 'goal', 'of', 'extracting', 'useful', 'knowledge', 'from', 'the', 'data'], ['Data', 'Science', 'is', 'not', 'restricted', 'to', 'only', 'big', 'data', 'although', 'the', 'fact', 'that', 'data', 'is', 'scaling', 'up', 'makes', 'big', 'data', 'an', 'important', 'aspect', 'of', 'data', 'science'], ['A', 'practitioner', 'of', 'data', 'science', 'is', 'called', 'a', 'data', 'scientist'], ['Data', 'scientists', 'solve', 'complex', 'data', 'problems', 'using', 'various', 'elements', 'of', 'mathematics', 'statistics', 'and', 'computer', 'science', 'although', 'expertise', 'in', 'these', 'subjects', 'are', 'not', 'required'], ['However', 'a', 'data', 'scientist', 'is', 'most', 'likely', 'to', 'be', 'an', 'expert', 'in', 'only', 'one', 'or', 'two', 'of', 'these', 'disciplines', 'meaning', 'that', 'cross', 'disciplinary', 'teams', 'can', 'be', 'a', 'key', 'component', 'of', 'data', 'science'], ['Good', 'data', 'scientists', 'are', 'able', 'to', 'apply', 'their', 'skills', 'to', 'achieve', 'a', 'broad', 'spectrum', 'of', 'end', 'results'], ['The', 'skill-sets', 'and', 'competencies', 'that', 'data', 'scientists', 'employ', 'vary', 'widely']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "word_tokenized_filtered = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized_sent = word_tokenize(s)\n",
    "    word_tokenized_filtered.append([w for w in word_tokenized_sent if w.lower() not in PunktSentenceTokenizer.PUNCTUATION])\n",
    "\n",
    "print(word_tokenized_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 5:** Dado un fichero de texto con contenido en inglés (\"Data_Science.txt\") se quiere saber cuáles son las 5 palabras más frecuentes del texto, antes y después de eliminar los signos de puntuación. Además, se quiere saber cuántas veces aparece la palabra \"data\" en el texto.  \n",
    "Se puede consultar la API de NLTK: https://www.nltk.org/api/nltk.probability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'science', 'is', 'the', 'study', 'of', 'the', 'extraction', 'of', 'knowledge', 'from', 'data', '.'], ['It', 'uses', 'various', 'techniques', 'from', 'many', 'fields', ',', 'including', 'signal', 'processing', ',', 'mathematics', ',', 'probability', 'models', ',', 'machine', 'learning', ',', 'computer', 'programming', ',', 'statistics', ',', 'data', 'engineering', ',', 'pattern', 'recognition', 'and', 'learning', ',', 'visualization', ',', 'uncertainty', 'modeling', ',', 'data', 'warehousing', ',', 'and', 'high', 'performance', 'computing', 'with', 'the', 'goal', 'of', 'extracting', 'useful', 'knowledge', 'from', 'the', 'data', '.'], ['Data', 'Science', 'is', 'not', 'restricted', 'to', 'only', 'big', 'data', ',', 'although', 'the', 'fact', 'that', 'data', 'is', 'scaling', 'up', 'makes', 'big', 'data', 'an', 'important', 'aspect', 'of', 'data', 'science', '.'], ['A', 'practitioner', 'of', 'data', 'science', 'is', 'called', 'a', 'data', 'scientist', '.'], ['Data', 'scientists', 'solve', 'complex', 'data', 'problems', 'using', 'various', 'elements', 'of', 'mathematics', ',', 'statistics', 'and', 'computer', 'science', ',', 'although', 'expertise', 'in', 'these', 'subjects', 'are', 'not', 'required', '.'], ['However', ',', 'a', 'data', 'scientist', 'is', 'most', 'likely', 'to', 'be', 'an', 'expert', 'in', 'only', 'one', 'or', 'two', 'of', 'these', 'disciplines', ',', 'meaning', 'that', 'cross', 'disciplinary', 'teams', 'can', 'be', 'a', 'key', 'component', 'of', 'data', 'science', '.'], ['Good', 'data', 'scientists', 'are', 'able', 'to', 'apply', 'their', 'skills', 'to', 'achieve', 'a', 'broad', 'spectrum', 'of', 'end', 'results', '.'], ['The', 'skill-sets', 'and', 'competencies', 'that', 'data', 'scientists', 'employ', 'vary', 'widely', '.']]\n",
      "[['DatascienceisthestudyoftheextractionofknowledgefromdataItusesvarioustechniquesfrommanyfieldsincludingsignalprocessingmathematicsprobabilitymodelsmachinelearningcomputerprogrammingstatisticsdataengineeringpatternrecognitionandlearningvisualizationuncertaintymodelingdatawarehousingandhighperformancecomputingwiththegoalofextractingusefulknowledgefromthedataDataScienceisnotrestrictedtoonlybigdataalthoughthefactthatdataisscalingupmakesbigdataanimportantaspectofdatascienceApractitionerofdatascienceiscalledadatascientistDatascientistssolvecomplexdataproblemsusingvariouselementsofmathematicsstatisticsandcomputersciencealthoughexpertiseinthesesubjectsarenotrequiredHoweveradatascientistismostlikelytobeanexpertinonlyoneortwoofthesedisciplinesmeaningthatcrossdisciplinaryteamscanbeakeycomponentofdatascienceGooddatascientistsareabletoapplytheirskillstoachieveabroadspectrumofendresultsTheskill-setsandcompetenciesthatdatascientistsemployvarywidely']]\n"
     ]
    }
   ],
   "source": [
    "# Leemos el fichero \"Data_Science.txt\" y lo almacenamos en una variable\n",
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    data_science = text.read()\n",
    "    sent_tokenized1 = PunktSentenceTokenizer().tokenize(data_science)\n",
    "    word_tokenized1 = []\n",
    "    for sentence in sent_tokenized1:\n",
    "        word_tokenized1.append(word_tokenize(sentence))\n",
    "\n",
    "# Leemos el fichero \"Data_Science_no_punct.txt\" y lo almacenamos en una variable\n",
    "with open(\"data/Data_Science_no_punct.txt\", \"r\") as text:\n",
    "    data_science_no_punct = text.read()\n",
    "    sent_tokenized2 = PunktSentenceTokenizer().tokenize(data_science_no_punct)\n",
    "    word_tokenized2 = []\n",
    "    for sentence in sent_tokenized2:\n",
    "        word_tokenized2.append(word_tokenize(sentence))\n",
    "\n",
    "print(word_tokenized1)\n",
    "print(word_tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 5 palabras más usadas (con signos de puntuación): ***\n",
      "[(',', 17), ('data', 15), ('of', 9), ('.', 8), ('science', 5), ('is', 5)]\n"
     ]
    }
   ],
   "source": [
    "fdist1 = FreqDist()\n",
    "for s in word_tokenized1:\n",
    "    for w in s:\n",
    "        fdist1[w] += 1\n",
    "\n",
    "print(\"*** 5 palabras más usadas (con signos de puntuación): ***\")\n",
    "print(fdist1.most_common(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 5 palabras más usadas (sin signos de puntuación): ***\n",
      "[(',', 17), ('data', 15), ('of', 9), ('.', 8), ('science', 5), ('is', 5)]\n"
     ]
    }
   ],
   "source": [
    "fdist2 = FreqDist()\n",
    "for s in word_tokenized1:\n",
    "    for w in s:\n",
    "        fdist2[w] += 1\n",
    "\n",
    "print(\"*** 5 palabras más usadas (sin signos de puntuación): ***\")\n",
    "print(fdist2.most_common(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra data aparece 15 veces\n"
     ]
    }
   ],
   "source": [
    "print(f\"La palabra data aparece {fdist1[\"data\"]} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 6:** Dado un fichero de texto con contenido en inglés (“Data_Science.txt”) se quiere obtener su contenido, dividirlo en frases y por cada una de ellas eliminar las palabras vacías (stop words) que contengan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'science', 'study', 'extraction', 'knowledge', 'data', '.']\n",
      "['uses', 'various', 'techniques', 'many', 'fields', ',', 'including', 'signal', 'processing', ',', 'mathematics', ',', 'probability', 'models', ',', 'machine', 'learning', ',', 'computer', 'programming', ',', 'statistics', ',', 'data', 'engineering', ',', 'pattern', 'recognition', 'learning', ',', 'visualization', ',', 'uncertainty', 'modeling', ',', 'data', 'warehousing', ',', 'high', 'performance', 'computing', 'goal', 'extracting', 'useful', 'knowledge', 'data', '.']\n",
      "['Data', 'Science', 'restricted', 'big', 'data', ',', 'although', 'fact', 'data', 'scaling', 'makes', 'big', 'data', 'important', 'aspect', 'data', 'science', '.']\n",
      "['practitioner', 'data', 'science', 'called', 'data', 'scientist', '.']\n",
      "['Data', 'scientists', 'solve', 'complex', 'data', 'problems', 'using', 'various', 'elements', 'mathematics', ',', 'statistics', 'computer', 'science', ',', 'although', 'expertise', 'subjects', 'required', '.']\n",
      "['However', ',', 'data', 'scientist', 'likely', 'expert', 'one', 'two', 'disciplines', ',', 'meaning', 'cross', 'disciplinary', 'teams', 'key', 'component', 'data', 'science', '.']\n",
      "['Good', 'data', 'scientists', 'able', 'apply', 'skills', 'achieve', 'broad', 'spectrum', 'end', 'results', '.']\n",
      "['skill-sets', 'competencies', 'data', 'scientists', 'employ', 'vary', 'widely', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_tokenized_filtered = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized_sent = word_tokenize(s)\n",
    "    word_tokenized_filtered.append([w for w in word_tokenized_sent if w.lower() not in stop_words])\n",
    "\n",
    "for s in word_tokenized_filtered:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 7:** Se pide lo mismo que en el ejercicio anterior, salvo que hay que hacerlo para un texto en español (“Ciencia_de_datos.txt”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['personas', 'dedican', 'ciencia', 'datos', 'conoce', 'científico', 'datos', ',', 'acuerdo', 'proyecto', 'Master', 'in', 'Data', 'Science', 'define', 'científico', 'datos', 'mezcla', 'estadísticos', ',', 'computólogos', 'pensadores', 'creativos', ',', 'siguientes', 'habilidades', ':', 'Recopilar', ',', 'procesar', 'extraer', 'valor', 'diversas', 'extensas', 'bases', 'datos', '.']\n",
      "['Imaginación', 'comprender', ',', 'visualizar', 'comunicar', 'conclusiones', 'científicos', 'datos', '.']\n",
      "['Capacidad', 'crear', 'soluciones', 'basadas', 'datos', 'aumentan', 'beneficios', ',', 'reducen', 'costos', '.']\n",
      "['científicos', 'datos', 'trabajan', 'todas', 'industrias', 'hacen', 'frente', 'grandes', 'proyectos', 'datos', 'niveles', '.']\n",
      "['doctor', 'estadística', 'Nathan', 'Yau', ',', 'precisó', 'siguiente', ':', 'científico', 'datos', 'estadístico', 'debería', 'aprender', 'interfaces', 'programación', 'aplicaciones', '(', 'APIs', ')', ',', 'bases', 'datos', 'extracción', 'datos', ';', 'diseñador', 'deberá', 'aprender', 'programar', ';', 'computólogo', 'deberá', 'saber', 'analizar', 'encontrar', 'datos', 'significado', '.']\n",
      "['6', 'tesis', 'doctoral', 'Benjamin', 'Fry', 'explicó', 'proceso', 'comprender', 'mejor', 'datos', 'comenzaba', 'serie', 'números', 'objetivo', 'responder', 'preguntas', 'datos', ',', 'cada', 'fase', 'proceso', 'propone', '(', 'adquirir', ',', 'analizar', ',', 'filtrar', ',', 'extraer', ',', 'representar', ',', 'refinar', 'interactuar', ')', ',', 'requiere', 'diferentes', 'enfoques', 'especializados', 'aporten', 'mejor', 'comprensión', 'datos', '.']\n",
      "['enfoques', 'menciona', 'Fry', ':', 'ingenieros', 'sistemas', ',', 'matemáticos', ',', 'estadísticos', ',', 'diseñadores', 'gráficos', ',', 'especialistas', 'visualización', 'información', 'especialistas', 'interacciones', 'hombre-máquina', ',', 'mejor', 'conocidos', 'siglas', 'inglés', '“', 'HCI', '”', '(', 'Human-Computer', 'Interaction', ')', '.']\n",
      "['Además', ',', 'Fry', 'afirmó', 'contar', 'diferentes', 'enfoques', 'especializados', 'lejos', 'resolver', 'problema', 'entendimiento', 'datos', ',', 'convierte', 'parte', 'problema', ',', 'cada', 'especialización', 'conduce', 'manera', 'aislada', 'problema', 'camino', 'hacia', 'solución', 'puede', 'perder', 'cada', 'transición', 'proceso', '.']\n",
      "['7', ':', 'Drew', 'Conway', 'página', 'web', 'explica', 'ayuda', 'diagrama', 'Venn', ',', 'principales', 'habilidades', 'dan', 'vida', 'forma', 'ciencia', 'datos', ',', 'así', 'relaciones', 'conjuntos', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Ciencia_de_datos.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "word_tokenized_filtered = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized_sent = word_tokenize(s)\n",
    "    word_tokenized_filtered.append([w for w in word_tokenized_sent if w.lower() not in stop_words])\n",
    "\n",
    "for s in word_tokenized_filtered:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 8:** Dado un fichero de texto con contenido en inglés (“Data_Science.txt”) se quiere hacer uso de expresiones regulares para encontrar palabras que cumplan unas determinadas condiciones en el texto, por ejemplo:\n",
    "    - Buscar las palabras que comiencen por ‘d’.\n",
    "    - Buscar las palabras que comiencen por ‘s’, terminen por ‘e’ y cuya longitud total sea de 7 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'data'], ['data', 'data', 'data'], ['Data', 'data', 'data', 'data', 'data'], ['data', 'data'], ['Data', 'data'], ['data', 'disciplines', 'disciplinary', 'data'], ['data'], ['data']]\n",
      "[['science'], [], ['Science', 'science'], ['science'], ['science'], ['science'], [], []]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Data_Science.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "sent_tokenized = PunktSentenceTokenizer().tokenize(text)\n",
    "word_tok1 = [regexp_tokenize(s, r\"\\b[dD]\\w*\\b\") for s in sent_tokenized] # palabras que comienzan por \"d\"\n",
    "word_tok2 = [regexp_tokenize(s, r\"\\b[sS]\\w{5}e\\b\") for s in sent_tokenized] # palabras que comienzan por \"d\"\n",
    "print(word_tok1)\n",
    "print(word_tok2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "- **Ejercicio 9:** Dada una frase en inglés, realizar un análisis morfológico obteniendo la etiqueta del Part-of-Speech para cada token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'), (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"John's big idea isn't all that bad.\"\n",
    "print(pos_tag(word_tokenize(sent), tagset=\"universal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 10:** Dado un fichero de texto con contenido en inglés (“Data_Science.txt”) se pide obtener los diferentes tokens del texto y por cada uno de ellos realizar stemming. Observa la salida, ¿Todos los stems obtenidos están en el diccionario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia 0:\n",
      "Token: Data; Stem: data\n",
      "Token: science; Stem: scienc\n",
      "Token: is; Stem: is\n",
      "Token: the; Stem: the\n",
      "Token: study; Stem: studi\n",
      "Token: of; Stem: of\n",
      "Token: the; Stem: the\n",
      "Token: extraction; Stem: extract\n",
      "Token: of; Stem: of\n",
      "Token: knowledge; Stem: knowledg\n",
      "Token: from; Stem: from\n",
      "Token: data; Stem: data\n",
      "Token: .; Stem: .\n",
      "Sentencia 1:\n",
      "Token: It; Stem: it\n",
      "Token: uses; Stem: use\n",
      "Token: various; Stem: variou\n",
      "Token: techniques; Stem: techniqu\n",
      "Token: from; Stem: from\n",
      "Token: many; Stem: mani\n",
      "Token: fields; Stem: field\n",
      "Token: ,; Stem: ,\n",
      "Token: including; Stem: includ\n",
      "Token: signal; Stem: signal\n",
      "Token: processing; Stem: process\n",
      "Token: ,; Stem: ,\n",
      "Token: mathematics; Stem: mathemat\n",
      "Token: ,; Stem: ,\n",
      "Token: probability; Stem: probabl\n",
      "Token: models; Stem: model\n",
      "Token: ,; Stem: ,\n",
      "Token: machine; Stem: machin\n",
      "Token: learning; Stem: learn\n",
      "Token: ,; Stem: ,\n",
      "Token: computer; Stem: comput\n",
      "Token: programming; Stem: program\n",
      "Token: ,; Stem: ,\n",
      "Token: statistics; Stem: statist\n",
      "Token: ,; Stem: ,\n",
      "Token: data; Stem: data\n",
      "Token: engineering; Stem: engin\n",
      "Token: ,; Stem: ,\n",
      "Token: pattern; Stem: pattern\n",
      "Token: recognition; Stem: recognit\n",
      "Token: and; Stem: and\n",
      "Token: learning; Stem: learn\n",
      "Token: ,; Stem: ,\n",
      "Token: visualization; Stem: visual\n",
      "Token: ,; Stem: ,\n",
      "Token: uncertainty; Stem: uncertainti\n",
      "Token: modeling; Stem: model\n",
      "Token: ,; Stem: ,\n",
      "Token: data; Stem: data\n",
      "Token: warehousing; Stem: wareh\n",
      "Token: ,; Stem: ,\n",
      "Token: and; Stem: and\n",
      "Token: high; Stem: high\n",
      "Token: performance; Stem: perform\n",
      "Token: computing; Stem: comput\n",
      "Token: with; Stem: with\n",
      "Token: the; Stem: the\n",
      "Token: goal; Stem: goal\n",
      "Token: of; Stem: of\n",
      "Token: extracting; Stem: extract\n",
      "Token: useful; Stem: use\n",
      "Token: knowledge; Stem: knowledg\n",
      "Token: from; Stem: from\n",
      "Token: the; Stem: the\n",
      "Token: data; Stem: data\n",
      "Token: .; Stem: .\n",
      "Sentencia 2:\n",
      "Token: Data; Stem: data\n",
      "Token: Science; Stem: scienc\n",
      "Token: is; Stem: is\n",
      "Token: not; Stem: not\n",
      "Token: restricted; Stem: restrict\n",
      "Token: to; Stem: to\n",
      "Token: only; Stem: onli\n",
      "Token: big; Stem: big\n",
      "Token: data; Stem: data\n",
      "Token: ,; Stem: ,\n",
      "Token: although; Stem: although\n",
      "Token: the; Stem: the\n",
      "Token: fact; Stem: fact\n",
      "Token: that; Stem: that\n",
      "Token: data; Stem: data\n",
      "Token: is; Stem: is\n",
      "Token: scaling; Stem: scale\n",
      "Token: up; Stem: up\n",
      "Token: makes; Stem: make\n",
      "Token: big; Stem: big\n",
      "Token: data; Stem: data\n",
      "Token: an; Stem: an\n",
      "Token: important; Stem: import\n",
      "Token: aspect; Stem: aspect\n",
      "Token: of; Stem: of\n",
      "Token: data; Stem: data\n",
      "Token: science; Stem: scienc\n",
      "Token: .; Stem: .\n",
      "Sentencia 3:\n",
      "Token: A; Stem: a\n",
      "Token: practitioner; Stem: practition\n",
      "Token: of; Stem: of\n",
      "Token: data; Stem: data\n",
      "Token: science; Stem: scienc\n",
      "Token: is; Stem: is\n",
      "Token: called; Stem: call\n",
      "Token: a; Stem: a\n",
      "Token: data; Stem: data\n",
      "Token: scientist; Stem: scientist\n",
      "Token: .; Stem: .\n",
      "Sentencia 4:\n",
      "Token: Data; Stem: data\n",
      "Token: scientists; Stem: scientist\n",
      "Token: solve; Stem: solv\n",
      "Token: complex; Stem: complex\n",
      "Token: data; Stem: data\n",
      "Token: problems; Stem: problem\n",
      "Token: using; Stem: use\n",
      "Token: various; Stem: variou\n",
      "Token: elements; Stem: element\n",
      "Token: of; Stem: of\n",
      "Token: mathematics; Stem: mathemat\n",
      "Token: ,; Stem: ,\n",
      "Token: statistics; Stem: statist\n",
      "Token: and; Stem: and\n",
      "Token: computer; Stem: comput\n",
      "Token: science; Stem: scienc\n",
      "Token: ,; Stem: ,\n",
      "Token: although; Stem: although\n",
      "Token: expertise; Stem: expertis\n",
      "Token: in; Stem: in\n",
      "Token: these; Stem: these\n",
      "Token: subjects; Stem: subject\n",
      "Token: are; Stem: are\n",
      "Token: not; Stem: not\n",
      "Token: required; Stem: requir\n",
      "Token: .; Stem: .\n",
      "Sentencia 5:\n",
      "Token: However; Stem: howev\n",
      "Token: ,; Stem: ,\n",
      "Token: a; Stem: a\n",
      "Token: data; Stem: data\n",
      "Token: scientist; Stem: scientist\n",
      "Token: is; Stem: is\n",
      "Token: most; Stem: most\n",
      "Token: likely; Stem: like\n",
      "Token: to; Stem: to\n",
      "Token: be; Stem: be\n",
      "Token: an; Stem: an\n",
      "Token: expert; Stem: expert\n",
      "Token: in; Stem: in\n",
      "Token: only; Stem: onli\n",
      "Token: one; Stem: one\n",
      "Token: or; Stem: or\n",
      "Token: two; Stem: two\n",
      "Token: of; Stem: of\n",
      "Token: these; Stem: these\n",
      "Token: disciplines; Stem: disciplin\n",
      "Token: ,; Stem: ,\n",
      "Token: meaning; Stem: mean\n",
      "Token: that; Stem: that\n",
      "Token: cross; Stem: cross\n",
      "Token: disciplinary; Stem: disciplinari\n",
      "Token: teams; Stem: team\n",
      "Token: can; Stem: can\n",
      "Token: be; Stem: be\n",
      "Token: a; Stem: a\n",
      "Token: key; Stem: key\n",
      "Token: component; Stem: compon\n",
      "Token: of; Stem: of\n",
      "Token: data; Stem: data\n",
      "Token: science; Stem: scienc\n",
      "Token: .; Stem: .\n",
      "Sentencia 6:\n",
      "Token: Good; Stem: good\n",
      "Token: data; Stem: data\n",
      "Token: scientists; Stem: scientist\n",
      "Token: are; Stem: are\n",
      "Token: able; Stem: abl\n",
      "Token: to; Stem: to\n",
      "Token: apply; Stem: appli\n",
      "Token: their; Stem: their\n",
      "Token: skills; Stem: skill\n",
      "Token: to; Stem: to\n",
      "Token: achieve; Stem: achiev\n",
      "Token: a; Stem: a\n",
      "Token: broad; Stem: broad\n",
      "Token: spectrum; Stem: spectrum\n",
      "Token: of; Stem: of\n",
      "Token: end; Stem: end\n",
      "Token: results; Stem: result\n",
      "Token: .; Stem: .\n",
      "Sentencia 7:\n",
      "Token: The; Stem: the\n",
      "Token: skill-sets; Stem: skill-set\n",
      "Token: and; Stem: and\n",
      "Token: competencies; Stem: compet\n",
      "Token: that; Stem: that\n",
      "Token: data; Stem: data\n",
      "Token: scientists; Stem: scientist\n",
      "Token: employ; Stem: employ\n",
      "Token: vary; Stem: vari\n",
      "Token: widely; Stem: wide\n",
      "Token: .; Stem: .\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized.append(word_tokenize(s))\n",
    "\n",
    "for s_idx, s in enumerate(word_tokenized):\n",
    "    print(f\"Sentencia {s_idx}:\")\n",
    "    for w in s:\n",
    "        print(f\"Token: {w}; Stem: {PorterStemmer().stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 11:** Se pide lo mismo que en el ejercicio anterior, salvo que hay que hacerlo para un texto en español (“Ciencia_de_datos.txt”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia 0:\n",
      "Token: Las; Stem: la\n",
      "Token: personas; Stem: persona\n",
      "Token: que; Stem: que\n",
      "Token: se; Stem: se\n",
      "Token: dedican; Stem: dedican\n",
      "Token: a; Stem: a\n",
      "Token: la; Stem: la\n",
      "Token: ciencia; Stem: ciencia\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: se; Stem: se\n",
      "Token: les; Stem: le\n",
      "Token: conoce; Stem: conoc\n",
      "Token: como; Stem: como\n",
      "Token: científico; Stem: científico\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: ,; Stem: ,\n",
      "Token: de; Stem: de\n",
      "Token: acuerdo; Stem: acuerdo\n",
      "Token: con; Stem: con\n",
      "Token: el; Stem: el\n",
      "Token: proyecto; Stem: proyecto\n",
      "Token: Master; Stem: master\n",
      "Token: in; Stem: in\n",
      "Token: Data; Stem: data\n",
      "Token: Science; Stem: scienc\n",
      "Token: define; Stem: defin\n",
      "Token: al; Stem: al\n",
      "Token: científico; Stem: científico\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: como; Stem: como\n",
      "Token: una; Stem: una\n",
      "Token: mezcla; Stem: mezcla\n",
      "Token: de; Stem: de\n",
      "Token: estadísticos; Stem: estadístico\n",
      "Token: ,; Stem: ,\n",
      "Token: computólogos; Stem: computólogo\n",
      "Token: y; Stem: y\n",
      "Token: pensadores; Stem: pensador\n",
      "Token: creativos; Stem: creativo\n",
      "Token: ,; Stem: ,\n",
      "Token: con; Stem: con\n",
      "Token: las; Stem: la\n",
      "Token: siguientes; Stem: siguient\n",
      "Token: habilidades; Stem: habilidad\n",
      "Token: :; Stem: :\n",
      "Token: Recopilar; Stem: recopilar\n",
      "Token: ,; Stem: ,\n",
      "Token: procesar; Stem: procesar\n",
      "Token: y; Stem: y\n",
      "Token: extraer; Stem: extraer\n",
      "Token: valor; Stem: valor\n",
      "Token: de; Stem: de\n",
      "Token: las; Stem: la\n",
      "Token: diversas; Stem: diversa\n",
      "Token: y; Stem: y\n",
      "Token: extensas; Stem: extensa\n",
      "Token: bases; Stem: base\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: .; Stem: .\n",
      "Sentencia 1:\n",
      "Token: Imaginación; Stem: imaginación\n",
      "Token: para; Stem: para\n",
      "Token: comprender; Stem: comprend\n",
      "Token: ,; Stem: ,\n",
      "Token: visualizar; Stem: visualizar\n",
      "Token: y; Stem: y\n",
      "Token: comunicar; Stem: comunicar\n",
      "Token: sus; Stem: su\n",
      "Token: conclusiones; Stem: conclusion\n",
      "Token: a; Stem: a\n",
      "Token: los; Stem: lo\n",
      "Token: no; Stem: no\n",
      "Token: científicos; Stem: científico\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: .; Stem: .\n",
      "Sentencia 2:\n",
      "Token: Capacidad; Stem: capacidad\n",
      "Token: para; Stem: para\n",
      "Token: crear; Stem: crear\n",
      "Token: soluciones; Stem: solucion\n",
      "Token: basadas; Stem: basada\n",
      "Token: en; Stem: en\n",
      "Token: datos; Stem: dato\n",
      "Token: que; Stem: que\n",
      "Token: aumentan; Stem: aumentan\n",
      "Token: los; Stem: lo\n",
      "Token: beneficios; Stem: beneficio\n",
      "Token: ,; Stem: ,\n",
      "Token: reducen; Stem: reducen\n",
      "Token: los; Stem: lo\n",
      "Token: costos; Stem: costo\n",
      "Token: .; Stem: .\n",
      "Sentencia 3:\n",
      "Token: Los; Stem: lo\n",
      "Token: científicos; Stem: científico\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: trabajan; Stem: trabajan\n",
      "Token: en; Stem: en\n",
      "Token: todas; Stem: toda\n",
      "Token: las; Stem: la\n",
      "Token: industrias; Stem: industria\n",
      "Token: y; Stem: y\n",
      "Token: hacen; Stem: hacen\n",
      "Token: frente; Stem: frent\n",
      "Token: a; Stem: a\n",
      "Token: los; Stem: lo\n",
      "Token: grandes; Stem: grand\n",
      "Token: proyectos; Stem: proyecto\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: en; Stem: en\n",
      "Token: todos; Stem: todo\n",
      "Token: los; Stem: lo\n",
      "Token: niveles; Stem: nivel\n",
      "Token: .; Stem: .\n",
      "Sentencia 4:\n",
      "Token: El; Stem: el\n",
      "Token: doctor; Stem: doctor\n",
      "Token: en; Stem: en\n",
      "Token: estadística; Stem: estadística\n",
      "Token: Nathan; Stem: nathan\n",
      "Token: Yau; Stem: yau\n",
      "Token: ,; Stem: ,\n",
      "Token: precisó; Stem: precisó\n",
      "Token: lo; Stem: lo\n",
      "Token: siguiente; Stem: siguient\n",
      "Token: :; Stem: :\n",
      "Token: el; Stem: el\n",
      "Token: científico; Stem: científico\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: es; Stem: es\n",
      "Token: un; Stem: un\n",
      "Token: estadístico; Stem: estadístico\n",
      "Token: que; Stem: que\n",
      "Token: debería; Stem: debería\n",
      "Token: aprender; Stem: aprend\n",
      "Token: interfaces; Stem: interfac\n",
      "Token: de; Stem: de\n",
      "Token: programación; Stem: programación\n",
      "Token: de; Stem: de\n",
      "Token: aplicaciones; Stem: aplicacion\n",
      "Token: (; Stem: (\n",
      "Token: APIs; Stem: api\n",
      "Token: ); Stem: )\n",
      "Token: ,; Stem: ,\n",
      "Token: bases; Stem: base\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: y; Stem: y\n",
      "Token: extracción; Stem: extracción\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: ;; Stem: ;\n",
      "Token: es; Stem: es\n",
      "Token: un; Stem: un\n",
      "Token: diseñador; Stem: diseñador\n",
      "Token: que; Stem: que\n",
      "Token: deberá; Stem: deberá\n",
      "Token: aprender; Stem: aprend\n",
      "Token: a; Stem: a\n",
      "Token: programar; Stem: programar\n",
      "Token: ;; Stem: ;\n",
      "Token: y; Stem: y\n",
      "Token: es; Stem: es\n",
      "Token: un; Stem: un\n",
      "Token: computólogo; Stem: computólogo\n",
      "Token: que; Stem: que\n",
      "Token: deberá; Stem: deberá\n",
      "Token: saber; Stem: saber\n",
      "Token: analizar; Stem: analizar\n",
      "Token: y; Stem: y\n",
      "Token: encontrar; Stem: encontrar\n",
      "Token: datos; Stem: dato\n",
      "Token: con; Stem: con\n",
      "Token: significado; Stem: significado\n",
      "Token: .; Stem: .\n",
      "Sentencia 5:\n",
      "Token: 6; Stem: 6\n",
      "Token: En; Stem: en\n",
      "Token: la; Stem: la\n",
      "Token: tesis; Stem: tesi\n",
      "Token: doctoral; Stem: doctor\n",
      "Token: de; Stem: de\n",
      "Token: Benjamin; Stem: benjamin\n",
      "Token: Fry; Stem: fri\n",
      "Token: explicó; Stem: explicó\n",
      "Token: que; Stem: que\n",
      "Token: el; Stem: el\n",
      "Token: proceso; Stem: proceso\n",
      "Token: para; Stem: para\n",
      "Token: comprender; Stem: comprend\n",
      "Token: mejor; Stem: mejor\n",
      "Token: a; Stem: a\n",
      "Token: los; Stem: lo\n",
      "Token: datos; Stem: dato\n",
      "Token: comenzaba; Stem: comenzaba\n",
      "Token: con; Stem: con\n",
      "Token: una; Stem: una\n",
      "Token: serie; Stem: seri\n",
      "Token: de; Stem: de\n",
      "Token: números; Stem: número\n",
      "Token: y; Stem: y\n",
      "Token: el; Stem: el\n",
      "Token: objetivo; Stem: objetivo\n",
      "Token: de; Stem: de\n",
      "Token: responder; Stem: respond\n",
      "Token: preguntas; Stem: pregunta\n",
      "Token: sobre; Stem: sobr\n",
      "Token: los; Stem: lo\n",
      "Token: datos; Stem: dato\n",
      "Token: ,; Stem: ,\n",
      "Token: en; Stem: en\n",
      "Token: cada; Stem: cada\n",
      "Token: fase; Stem: fase\n",
      "Token: del; Stem: del\n",
      "Token: proceso; Stem: proceso\n",
      "Token: que; Stem: que\n",
      "Token: él; Stem: él\n",
      "Token: propone; Stem: propon\n",
      "Token: (; Stem: (\n",
      "Token: adquirir; Stem: adquirir\n",
      "Token: ,; Stem: ,\n",
      "Token: analizar; Stem: analizar\n",
      "Token: ,; Stem: ,\n",
      "Token: filtrar; Stem: filtrar\n",
      "Token: ,; Stem: ,\n",
      "Token: extraer; Stem: extraer\n",
      "Token: ,; Stem: ,\n",
      "Token: representar; Stem: representar\n",
      "Token: ,; Stem: ,\n",
      "Token: refinar; Stem: refinar\n",
      "Token: e; Stem: e\n",
      "Token: interactuar; Stem: interactuar\n",
      "Token: ); Stem: )\n",
      "Token: ,; Stem: ,\n",
      "Token: se; Stem: se\n",
      "Token: requiere; Stem: requier\n",
      "Token: de; Stem: de\n",
      "Token: diferentes; Stem: diferent\n",
      "Token: enfoques; Stem: enfoqu\n",
      "Token: especializados; Stem: especializado\n",
      "Token: que; Stem: que\n",
      "Token: aporten; Stem: aporten\n",
      "Token: a; Stem: a\n",
      "Token: una; Stem: una\n",
      "Token: mejor; Stem: mejor\n",
      "Token: comprensión; Stem: comprensión\n",
      "Token: de; Stem: de\n",
      "Token: los; Stem: lo\n",
      "Token: datos; Stem: dato\n",
      "Token: .; Stem: .\n",
      "Sentencia 6:\n",
      "Token: Entre; Stem: entr\n",
      "Token: los; Stem: lo\n",
      "Token: enfoques; Stem: enfoqu\n",
      "Token: que; Stem: que\n",
      "Token: menciona; Stem: menciona\n",
      "Token: Fry; Stem: fri\n",
      "Token: están; Stem: están\n",
      "Token: :; Stem: :\n",
      "Token: ingenieros; Stem: ingeniero\n",
      "Token: en; Stem: en\n",
      "Token: sistemas; Stem: sistema\n",
      "Token: ,; Stem: ,\n",
      "Token: matemáticos; Stem: matemático\n",
      "Token: ,; Stem: ,\n",
      "Token: estadísticos; Stem: estadístico\n",
      "Token: ,; Stem: ,\n",
      "Token: diseñadores; Stem: diseñador\n",
      "Token: gráficos; Stem: gráfico\n",
      "Token: ,; Stem: ,\n",
      "Token: especialistas; Stem: especialista\n",
      "Token: en; Stem: en\n",
      "Token: visualización; Stem: visualización\n",
      "Token: de; Stem: de\n",
      "Token: la; Stem: la\n",
      "Token: información; Stem: información\n",
      "Token: y; Stem: y\n",
      "Token: especialistas; Stem: especialista\n",
      "Token: en; Stem: en\n",
      "Token: interacciones; Stem: interaccion\n",
      "Token: hombre-máquina; Stem: hombre-máquina\n",
      "Token: ,; Stem: ,\n",
      "Token: mejor; Stem: mejor\n",
      "Token: conocidos; Stem: conocido\n",
      "Token: por; Stem: por\n",
      "Token: sus; Stem: su\n",
      "Token: siglas; Stem: sigla\n",
      "Token: en; Stem: en\n",
      "Token: inglés; Stem: inglé\n",
      "Token: “; Stem: “\n",
      "Token: HCI; Stem: hci\n",
      "Token: ”; Stem: ”\n",
      "Token: (; Stem: (\n",
      "Token: Human-Computer; Stem: human-comput\n",
      "Token: Interaction; Stem: interact\n",
      "Token: ); Stem: )\n",
      "Token: .; Stem: .\n",
      "Sentencia 7:\n",
      "Token: Además; Stem: ademá\n",
      "Token: ,; Stem: ,\n",
      "Token: Fry; Stem: fri\n",
      "Token: afirmó; Stem: afirmó\n",
      "Token: que; Stem: que\n",
      "Token: contar; Stem: contar\n",
      "Token: con; Stem: con\n",
      "Token: diferentes; Stem: diferent\n",
      "Token: enfoques; Stem: enfoqu\n",
      "Token: especializados; Stem: especializado\n",
      "Token: lejos; Stem: lejo\n",
      "Token: de; Stem: de\n",
      "Token: resolver; Stem: resolv\n",
      "Token: el; Stem: el\n",
      "Token: problema; Stem: problema\n",
      "Token: de; Stem: de\n",
      "Token: entendimiento; Stem: entendimiento\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: ,; Stem: ,\n",
      "Token: se; Stem: se\n",
      "Token: convierte; Stem: conviert\n",
      "Token: en; Stem: en\n",
      "Token: parte; Stem: part\n",
      "Token: del; Stem: del\n",
      "Token: problema; Stem: problema\n",
      "Token: ,; Stem: ,\n",
      "Token: ya; Stem: ya\n",
      "Token: que; Stem: que\n",
      "Token: cada; Stem: cada\n",
      "Token: especialización; Stem: especialización\n",
      "Token: conduce; Stem: conduc\n",
      "Token: de; Stem: de\n",
      "Token: manera; Stem: manera\n",
      "Token: aislada; Stem: aislada\n",
      "Token: el; Stem: el\n",
      "Token: problema; Stem: problema\n",
      "Token: y; Stem: y\n",
      "Token: el; Stem: el\n",
      "Token: camino; Stem: camino\n",
      "Token: hacia; Stem: hacia\n",
      "Token: la; Stem: la\n",
      "Token: solución; Stem: solución\n",
      "Token: se; Stem: se\n",
      "Token: puede; Stem: pued\n",
      "Token: perder; Stem: perder\n",
      "Token: algo; Stem: algo\n",
      "Token: en; Stem: en\n",
      "Token: cada; Stem: cada\n",
      "Token: transición; Stem: transición\n",
      "Token: del; Stem: del\n",
      "Token: proceso; Stem: proceso\n",
      "Token: .; Stem: .\n",
      "Sentencia 8:\n",
      "Token: 7; Stem: 7\n",
      "Token: en; Stem: en\n",
      "Token: :; Stem: :\n",
      "Token: Drew; Stem: drew\n",
      "Token: Conway; Stem: conway\n",
      "Token: en; Stem: en\n",
      "Token: su; Stem: su\n",
      "Token: página; Stem: página\n",
      "Token: web; Stem: web\n",
      "Token: explica; Stem: explica\n",
      "Token: con; Stem: con\n",
      "Token: la; Stem: la\n",
      "Token: ayuda; Stem: ayuda\n",
      "Token: de; Stem: de\n",
      "Token: un; Stem: un\n",
      "Token: diagrama; Stem: diagrama\n",
      "Token: de; Stem: de\n",
      "Token: Venn; Stem: venn\n",
      "Token: ,; Stem: ,\n",
      "Token: las; Stem: la\n",
      "Token: principales; Stem: principal\n",
      "Token: habilidades; Stem: habilidad\n",
      "Token: que; Stem: que\n",
      "Token: le; Stem: le\n",
      "Token: dan; Stem: dan\n",
      "Token: vida; Stem: vida\n",
      "Token: y; Stem: y\n",
      "Token: forma; Stem: forma\n",
      "Token: a; Stem: a\n",
      "Token: la; Stem: la\n",
      "Token: ciencia; Stem: ciencia\n",
      "Token: de; Stem: de\n",
      "Token: datos; Stem: dato\n",
      "Token: ,; Stem: ,\n",
      "Token: así; Stem: así\n",
      "Token: como; Stem: como\n",
      "Token: sus; Stem: su\n",
      "Token: relaciones; Stem: relacion\n",
      "Token: de; Stem: de\n",
      "Token: conjuntos; Stem: conjunto\n",
      "Token: .; Stem: .\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Ciencia_de_datos.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized.append(word_tokenize(s))\n",
    "\n",
    "for s_idx, s in enumerate(word_tokenized):\n",
    "    print(f\"Sentencia {s_idx}:\")\n",
    "    for w in s:\n",
    "        print(f\"Token: {w}; Stem: {PorterStemmer().stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 12:** Dado un fichero de texto con contenido en inglés (“Data_Science.txt”) se pide obtener los diferentes tokens del texto y por cada uno de ellos lematizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia 0:\n",
      "Token: Data; Lema: Data\n",
      "Token: science; Lema: science\n",
      "Token: is; Lema: is\n",
      "Token: the; Lema: the\n",
      "Token: study; Lema: study\n",
      "Token: of; Lema: of\n",
      "Token: the; Lema: the\n",
      "Token: extraction; Lema: extraction\n",
      "Token: of; Lema: of\n",
      "Token: knowledge; Lema: knowledge\n",
      "Token: from; Lema: from\n",
      "Token: data; Lema: data\n",
      "Token: .; Lema: .\n",
      "Sentencia 1:\n",
      "Token: It; Lema: It\n",
      "Token: uses; Lema: us\n",
      "Token: various; Lema: various\n",
      "Token: techniques; Lema: technique\n",
      "Token: from; Lema: from\n",
      "Token: many; Lema: many\n",
      "Token: fields; Lema: field\n",
      "Token: ,; Lema: ,\n",
      "Token: including; Lema: including\n",
      "Token: signal; Lema: signal\n",
      "Token: processing; Lema: processing\n",
      "Token: ,; Lema: ,\n",
      "Token: mathematics; Lema: mathematics\n",
      "Token: ,; Lema: ,\n",
      "Token: probability; Lema: probability\n",
      "Token: models; Lema: model\n",
      "Token: ,; Lema: ,\n",
      "Token: machine; Lema: machine\n",
      "Token: learning; Lema: learning\n",
      "Token: ,; Lema: ,\n",
      "Token: computer; Lema: computer\n",
      "Token: programming; Lema: programming\n",
      "Token: ,; Lema: ,\n",
      "Token: statistics; Lema: statistic\n",
      "Token: ,; Lema: ,\n",
      "Token: data; Lema: data\n",
      "Token: engineering; Lema: engineering\n",
      "Token: ,; Lema: ,\n",
      "Token: pattern; Lema: pattern\n",
      "Token: recognition; Lema: recognition\n",
      "Token: and; Lema: and\n",
      "Token: learning; Lema: learning\n",
      "Token: ,; Lema: ,\n",
      "Token: visualization; Lema: visualization\n",
      "Token: ,; Lema: ,\n",
      "Token: uncertainty; Lema: uncertainty\n",
      "Token: modeling; Lema: modeling\n",
      "Token: ,; Lema: ,\n",
      "Token: data; Lema: data\n",
      "Token: warehousing; Lema: warehousing\n",
      "Token: ,; Lema: ,\n",
      "Token: and; Lema: and\n",
      "Token: high; Lema: high\n",
      "Token: performance; Lema: performance\n",
      "Token: computing; Lema: computing\n",
      "Token: with; Lema: with\n",
      "Token: the; Lema: the\n",
      "Token: goal; Lema: goal\n",
      "Token: of; Lema: of\n",
      "Token: extracting; Lema: extracting\n",
      "Token: useful; Lema: useful\n",
      "Token: knowledge; Lema: knowledge\n",
      "Token: from; Lema: from\n",
      "Token: the; Lema: the\n",
      "Token: data; Lema: data\n",
      "Token: .; Lema: .\n",
      "Sentencia 2:\n",
      "Token: Data; Lema: Data\n",
      "Token: Science; Lema: Science\n",
      "Token: is; Lema: is\n",
      "Token: not; Lema: not\n",
      "Token: restricted; Lema: restricted\n",
      "Token: to; Lema: to\n",
      "Token: only; Lema: only\n",
      "Token: big; Lema: big\n",
      "Token: data; Lema: data\n",
      "Token: ,; Lema: ,\n",
      "Token: although; Lema: although\n",
      "Token: the; Lema: the\n",
      "Token: fact; Lema: fact\n",
      "Token: that; Lema: that\n",
      "Token: data; Lema: data\n",
      "Token: is; Lema: is\n",
      "Token: scaling; Lema: scaling\n",
      "Token: up; Lema: up\n",
      "Token: makes; Lema: make\n",
      "Token: big; Lema: big\n",
      "Token: data; Lema: data\n",
      "Token: an; Lema: an\n",
      "Token: important; Lema: important\n",
      "Token: aspect; Lema: aspect\n",
      "Token: of; Lema: of\n",
      "Token: data; Lema: data\n",
      "Token: science; Lema: science\n",
      "Token: .; Lema: .\n",
      "Sentencia 3:\n",
      "Token: A; Lema: A\n",
      "Token: practitioner; Lema: practitioner\n",
      "Token: of; Lema: of\n",
      "Token: data; Lema: data\n",
      "Token: science; Lema: science\n",
      "Token: is; Lema: is\n",
      "Token: called; Lema: called\n",
      "Token: a; Lema: a\n",
      "Token: data; Lema: data\n",
      "Token: scientist; Lema: scientist\n",
      "Token: .; Lema: .\n",
      "Sentencia 4:\n",
      "Token: Data; Lema: Data\n",
      "Token: scientists; Lema: scientist\n",
      "Token: solve; Lema: solve\n",
      "Token: complex; Lema: complex\n",
      "Token: data; Lema: data\n",
      "Token: problems; Lema: problem\n",
      "Token: using; Lema: using\n",
      "Token: various; Lema: various\n",
      "Token: elements; Lema: element\n",
      "Token: of; Lema: of\n",
      "Token: mathematics; Lema: mathematics\n",
      "Token: ,; Lema: ,\n",
      "Token: statistics; Lema: statistic\n",
      "Token: and; Lema: and\n",
      "Token: computer; Lema: computer\n",
      "Token: science; Lema: science\n",
      "Token: ,; Lema: ,\n",
      "Token: although; Lema: although\n",
      "Token: expertise; Lema: expertise\n",
      "Token: in; Lema: in\n",
      "Token: these; Lema: these\n",
      "Token: subjects; Lema: subject\n",
      "Token: are; Lema: are\n",
      "Token: not; Lema: not\n",
      "Token: required; Lema: required\n",
      "Token: .; Lema: .\n",
      "Sentencia 5:\n",
      "Token: However; Lema: However\n",
      "Token: ,; Lema: ,\n",
      "Token: a; Lema: a\n",
      "Token: data; Lema: data\n",
      "Token: scientist; Lema: scientist\n",
      "Token: is; Lema: is\n",
      "Token: most; Lema: most\n",
      "Token: likely; Lema: likely\n",
      "Token: to; Lema: to\n",
      "Token: be; Lema: be\n",
      "Token: an; Lema: an\n",
      "Token: expert; Lema: expert\n",
      "Token: in; Lema: in\n",
      "Token: only; Lema: only\n",
      "Token: one; Lema: one\n",
      "Token: or; Lema: or\n",
      "Token: two; Lema: two\n",
      "Token: of; Lema: of\n",
      "Token: these; Lema: these\n",
      "Token: disciplines; Lema: discipline\n",
      "Token: ,; Lema: ,\n",
      "Token: meaning; Lema: meaning\n",
      "Token: that; Lema: that\n",
      "Token: cross; Lema: cross\n",
      "Token: disciplinary; Lema: disciplinary\n",
      "Token: teams; Lema: team\n",
      "Token: can; Lema: can\n",
      "Token: be; Lema: be\n",
      "Token: a; Lema: a\n",
      "Token: key; Lema: key\n",
      "Token: component; Lema: component\n",
      "Token: of; Lema: of\n",
      "Token: data; Lema: data\n",
      "Token: science; Lema: science\n",
      "Token: .; Lema: .\n",
      "Sentencia 6:\n",
      "Token: Good; Lema: Good\n",
      "Token: data; Lema: data\n",
      "Token: scientists; Lema: scientist\n",
      "Token: are; Lema: are\n",
      "Token: able; Lema: able\n",
      "Token: to; Lema: to\n",
      "Token: apply; Lema: apply\n",
      "Token: their; Lema: their\n",
      "Token: skills; Lema: skill\n",
      "Token: to; Lema: to\n",
      "Token: achieve; Lema: achieve\n",
      "Token: a; Lema: a\n",
      "Token: broad; Lema: broad\n",
      "Token: spectrum; Lema: spectrum\n",
      "Token: of; Lema: of\n",
      "Token: end; Lema: end\n",
      "Token: results; Lema: result\n",
      "Token: .; Lema: .\n",
      "Sentencia 7:\n",
      "Token: The; Lema: The\n",
      "Token: skill-sets; Lema: skill-sets\n",
      "Token: and; Lema: and\n",
      "Token: competencies; Lema: competency\n",
      "Token: that; Lema: that\n",
      "Token: data; Lema: data\n",
      "Token: scientists; Lema: scientist\n",
      "Token: employ; Lema: employ\n",
      "Token: vary; Lema: vary\n",
      "Token: widely; Lema: widely\n",
      "Token: .; Lema: .\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Data_Science.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized.append(word_tokenize(s))\n",
    "\n",
    "for s_idx, s in enumerate(word_tokenized):\n",
    "    print(f\"Sentencia {s_idx}:\")\n",
    "    for w in s:\n",
    "        print(f\"Token: {w}; Lema: {WordNetLemmatizer().lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 13:** Se pide lo mismo que en el ejercicio anterior, salvo que hay que hacerlo para un texto en español (“Ciencia_de_datos.txt”). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia 0:\n",
      "Token: Las; Lema: Las\n",
      "Token: personas; Lema: persona\n",
      "Token: que; Lema: que\n",
      "Token: se; Lema: se\n",
      "Token: dedican; Lema: dedican\n",
      "Token: a; Lema: a\n",
      "Token: la; Lema: la\n",
      "Token: ciencia; Lema: ciencia\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: se; Lema: se\n",
      "Token: les; Lema: le\n",
      "Token: conoce; Lema: conoce\n",
      "Token: como; Lema: como\n",
      "Token: científico; Lema: científico\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: ,; Lema: ,\n",
      "Token: de; Lema: de\n",
      "Token: acuerdo; Lema: acuerdo\n",
      "Token: con; Lema: con\n",
      "Token: el; Lema: el\n",
      "Token: proyecto; Lema: proyecto\n",
      "Token: Master; Lema: Master\n",
      "Token: in; Lema: in\n",
      "Token: Data; Lema: Data\n",
      "Token: Science; Lema: Science\n",
      "Token: define; Lema: define\n",
      "Token: al; Lema: al\n",
      "Token: científico; Lema: científico\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: como; Lema: como\n",
      "Token: una; Lema: una\n",
      "Token: mezcla; Lema: mezcla\n",
      "Token: de; Lema: de\n",
      "Token: estadísticos; Lema: estadísticos\n",
      "Token: ,; Lema: ,\n",
      "Token: computólogos; Lema: computólogos\n",
      "Token: y; Lema: y\n",
      "Token: pensadores; Lema: pensadores\n",
      "Token: creativos; Lema: creativos\n",
      "Token: ,; Lema: ,\n",
      "Token: con; Lema: con\n",
      "Token: las; Lema: la\n",
      "Token: siguientes; Lema: siguientes\n",
      "Token: habilidades; Lema: habilidades\n",
      "Token: :; Lema: :\n",
      "Token: Recopilar; Lema: Recopilar\n",
      "Token: ,; Lema: ,\n",
      "Token: procesar; Lema: procesar\n",
      "Token: y; Lema: y\n",
      "Token: extraer; Lema: extraer\n",
      "Token: valor; Lema: valor\n",
      "Token: de; Lema: de\n",
      "Token: las; Lema: la\n",
      "Token: diversas; Lema: diversas\n",
      "Token: y; Lema: y\n",
      "Token: extensas; Lema: extensas\n",
      "Token: bases; Lema: base\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: .; Lema: .\n",
      "Sentencia 1:\n",
      "Token: Imaginación; Lema: Imaginación\n",
      "Token: para; Lema: para\n",
      "Token: comprender; Lema: comprender\n",
      "Token: ,; Lema: ,\n",
      "Token: visualizar; Lema: visualizar\n",
      "Token: y; Lema: y\n",
      "Token: comunicar; Lema: comunicar\n",
      "Token: sus; Lema: sus\n",
      "Token: conclusiones; Lema: conclusiones\n",
      "Token: a; Lema: a\n",
      "Token: los; Lema: los\n",
      "Token: no; Lema: no\n",
      "Token: científicos; Lema: científicos\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: .; Lema: .\n",
      "Sentencia 2:\n",
      "Token: Capacidad; Lema: Capacidad\n",
      "Token: para; Lema: para\n",
      "Token: crear; Lema: crear\n",
      "Token: soluciones; Lema: soluciones\n",
      "Token: basadas; Lema: basadas\n",
      "Token: en; Lema: en\n",
      "Token: datos; Lema: datos\n",
      "Token: que; Lema: que\n",
      "Token: aumentan; Lema: aumentan\n",
      "Token: los; Lema: los\n",
      "Token: beneficios; Lema: beneficios\n",
      "Token: ,; Lema: ,\n",
      "Token: reducen; Lema: reducen\n",
      "Token: los; Lema: los\n",
      "Token: costos; Lema: costos\n",
      "Token: .; Lema: .\n",
      "Sentencia 3:\n",
      "Token: Los; Lema: Los\n",
      "Token: científicos; Lema: científicos\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: trabajan; Lema: trabajan\n",
      "Token: en; Lema: en\n",
      "Token: todas; Lema: toda\n",
      "Token: las; Lema: la\n",
      "Token: industrias; Lema: industrias\n",
      "Token: y; Lema: y\n",
      "Token: hacen; Lema: hacen\n",
      "Token: frente; Lema: frente\n",
      "Token: a; Lema: a\n",
      "Token: los; Lema: los\n",
      "Token: grandes; Lema: grandes\n",
      "Token: proyectos; Lema: proyectos\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: en; Lema: en\n",
      "Token: todos; Lema: todos\n",
      "Token: los; Lema: los\n",
      "Token: niveles; Lema: niveles\n",
      "Token: .; Lema: .\n",
      "Sentencia 4:\n",
      "Token: El; Lema: El\n",
      "Token: doctor; Lema: doctor\n",
      "Token: en; Lema: en\n",
      "Token: estadística; Lema: estadística\n",
      "Token: Nathan; Lema: Nathan\n",
      "Token: Yau; Lema: Yau\n",
      "Token: ,; Lema: ,\n",
      "Token: precisó; Lema: precisó\n",
      "Token: lo; Lema: lo\n",
      "Token: siguiente; Lema: siguiente\n",
      "Token: :; Lema: :\n",
      "Token: el; Lema: el\n",
      "Token: científico; Lema: científico\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: es; Lema: e\n",
      "Token: un; Lema: un\n",
      "Token: estadístico; Lema: estadístico\n",
      "Token: que; Lema: que\n",
      "Token: debería; Lema: debería\n",
      "Token: aprender; Lema: aprender\n",
      "Token: interfaces; Lema: interface\n",
      "Token: de; Lema: de\n",
      "Token: programación; Lema: programación\n",
      "Token: de; Lema: de\n",
      "Token: aplicaciones; Lema: aplicaciones\n",
      "Token: (; Lema: (\n",
      "Token: APIs; Lema: APIs\n",
      "Token: ); Lema: )\n",
      "Token: ,; Lema: ,\n",
      "Token: bases; Lema: base\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: y; Lema: y\n",
      "Token: extracción; Lema: extracción\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: ;; Lema: ;\n",
      "Token: es; Lema: e\n",
      "Token: un; Lema: un\n",
      "Token: diseñador; Lema: diseñador\n",
      "Token: que; Lema: que\n",
      "Token: deberá; Lema: deberá\n",
      "Token: aprender; Lema: aprender\n",
      "Token: a; Lema: a\n",
      "Token: programar; Lema: programar\n",
      "Token: ;; Lema: ;\n",
      "Token: y; Lema: y\n",
      "Token: es; Lema: e\n",
      "Token: un; Lema: un\n",
      "Token: computólogo; Lema: computólogo\n",
      "Token: que; Lema: que\n",
      "Token: deberá; Lema: deberá\n",
      "Token: saber; Lema: saber\n",
      "Token: analizar; Lema: analizar\n",
      "Token: y; Lema: y\n",
      "Token: encontrar; Lema: encontrar\n",
      "Token: datos; Lema: datos\n",
      "Token: con; Lema: con\n",
      "Token: significado; Lema: significado\n",
      "Token: .; Lema: .\n",
      "Sentencia 5:\n",
      "Token: 6; Lema: 6\n",
      "Token: En; Lema: En\n",
      "Token: la; Lema: la\n",
      "Token: tesis; Lema: tesis\n",
      "Token: doctoral; Lema: doctoral\n",
      "Token: de; Lema: de\n",
      "Token: Benjamin; Lema: Benjamin\n",
      "Token: Fry; Lema: Fry\n",
      "Token: explicó; Lema: explicó\n",
      "Token: que; Lema: que\n",
      "Token: el; Lema: el\n",
      "Token: proceso; Lema: proceso\n",
      "Token: para; Lema: para\n",
      "Token: comprender; Lema: comprender\n",
      "Token: mejor; Lema: mejor\n",
      "Token: a; Lema: a\n",
      "Token: los; Lema: los\n",
      "Token: datos; Lema: datos\n",
      "Token: comenzaba; Lema: comenzaba\n",
      "Token: con; Lema: con\n",
      "Token: una; Lema: una\n",
      "Token: serie; Lema: serie\n",
      "Token: de; Lema: de\n",
      "Token: números; Lema: números\n",
      "Token: y; Lema: y\n",
      "Token: el; Lema: el\n",
      "Token: objetivo; Lema: objetivo\n",
      "Token: de; Lema: de\n",
      "Token: responder; Lema: responder\n",
      "Token: preguntas; Lema: preguntas\n",
      "Token: sobre; Lema: sobre\n",
      "Token: los; Lema: los\n",
      "Token: datos; Lema: datos\n",
      "Token: ,; Lema: ,\n",
      "Token: en; Lema: en\n",
      "Token: cada; Lema: cada\n",
      "Token: fase; Lema: fase\n",
      "Token: del; Lema: del\n",
      "Token: proceso; Lema: proceso\n",
      "Token: que; Lema: que\n",
      "Token: él; Lema: él\n",
      "Token: propone; Lema: propone\n",
      "Token: (; Lema: (\n",
      "Token: adquirir; Lema: adquirir\n",
      "Token: ,; Lema: ,\n",
      "Token: analizar; Lema: analizar\n",
      "Token: ,; Lema: ,\n",
      "Token: filtrar; Lema: filtrar\n",
      "Token: ,; Lema: ,\n",
      "Token: extraer; Lema: extraer\n",
      "Token: ,; Lema: ,\n",
      "Token: representar; Lema: representar\n",
      "Token: ,; Lema: ,\n",
      "Token: refinar; Lema: refinar\n",
      "Token: e; Lema: e\n",
      "Token: interactuar; Lema: interactuar\n",
      "Token: ); Lema: )\n",
      "Token: ,; Lema: ,\n",
      "Token: se; Lema: se\n",
      "Token: requiere; Lema: requiere\n",
      "Token: de; Lema: de\n",
      "Token: diferentes; Lema: diferentes\n",
      "Token: enfoques; Lema: enfoques\n",
      "Token: especializados; Lema: especializados\n",
      "Token: que; Lema: que\n",
      "Token: aporten; Lema: aporten\n",
      "Token: a; Lema: a\n",
      "Token: una; Lema: una\n",
      "Token: mejor; Lema: mejor\n",
      "Token: comprensión; Lema: comprensión\n",
      "Token: de; Lema: de\n",
      "Token: los; Lema: los\n",
      "Token: datos; Lema: datos\n",
      "Token: .; Lema: .\n",
      "Sentencia 6:\n",
      "Token: Entre; Lema: Entre\n",
      "Token: los; Lema: los\n",
      "Token: enfoques; Lema: enfoques\n",
      "Token: que; Lema: que\n",
      "Token: menciona; Lema: menciona\n",
      "Token: Fry; Lema: Fry\n",
      "Token: están; Lema: están\n",
      "Token: :; Lema: :\n",
      "Token: ingenieros; Lema: ingenieros\n",
      "Token: en; Lema: en\n",
      "Token: sistemas; Lema: sistemas\n",
      "Token: ,; Lema: ,\n",
      "Token: matemáticos; Lema: matemáticos\n",
      "Token: ,; Lema: ,\n",
      "Token: estadísticos; Lema: estadísticos\n",
      "Token: ,; Lema: ,\n",
      "Token: diseñadores; Lema: diseñadores\n",
      "Token: gráficos; Lema: gráficos\n",
      "Token: ,; Lema: ,\n",
      "Token: especialistas; Lema: especialistas\n",
      "Token: en; Lema: en\n",
      "Token: visualización; Lema: visualización\n",
      "Token: de; Lema: de\n",
      "Token: la; Lema: la\n",
      "Token: información; Lema: información\n",
      "Token: y; Lema: y\n",
      "Token: especialistas; Lema: especialistas\n",
      "Token: en; Lema: en\n",
      "Token: interacciones; Lema: interacciones\n",
      "Token: hombre-máquina; Lema: hombre-máquina\n",
      "Token: ,; Lema: ,\n",
      "Token: mejor; Lema: mejor\n",
      "Token: conocidos; Lema: conocidos\n",
      "Token: por; Lema: por\n",
      "Token: sus; Lema: sus\n",
      "Token: siglas; Lema: siglas\n",
      "Token: en; Lema: en\n",
      "Token: inglés; Lema: inglés\n",
      "Token: “; Lema: “\n",
      "Token: HCI; Lema: HCI\n",
      "Token: ”; Lema: ”\n",
      "Token: (; Lema: (\n",
      "Token: Human-Computer; Lema: Human-Computer\n",
      "Token: Interaction; Lema: Interaction\n",
      "Token: ); Lema: )\n",
      "Token: .; Lema: .\n",
      "Sentencia 7:\n",
      "Token: Además; Lema: Además\n",
      "Token: ,; Lema: ,\n",
      "Token: Fry; Lema: Fry\n",
      "Token: afirmó; Lema: afirmó\n",
      "Token: que; Lema: que\n",
      "Token: contar; Lema: contar\n",
      "Token: con; Lema: con\n",
      "Token: diferentes; Lema: diferentes\n",
      "Token: enfoques; Lema: enfoques\n",
      "Token: especializados; Lema: especializados\n",
      "Token: lejos; Lema: lejos\n",
      "Token: de; Lema: de\n",
      "Token: resolver; Lema: resolver\n",
      "Token: el; Lema: el\n",
      "Token: problema; Lema: problema\n",
      "Token: de; Lema: de\n",
      "Token: entendimiento; Lema: entendimiento\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: ,; Lema: ,\n",
      "Token: se; Lema: se\n",
      "Token: convierte; Lema: convierte\n",
      "Token: en; Lema: en\n",
      "Token: parte; Lema: parte\n",
      "Token: del; Lema: del\n",
      "Token: problema; Lema: problema\n",
      "Token: ,; Lema: ,\n",
      "Token: ya; Lema: ya\n",
      "Token: que; Lema: que\n",
      "Token: cada; Lema: cada\n",
      "Token: especialización; Lema: especialización\n",
      "Token: conduce; Lema: conduce\n",
      "Token: de; Lema: de\n",
      "Token: manera; Lema: manera\n",
      "Token: aislada; Lema: aislada\n",
      "Token: el; Lema: el\n",
      "Token: problema; Lema: problema\n",
      "Token: y; Lema: y\n",
      "Token: el; Lema: el\n",
      "Token: camino; Lema: camino\n",
      "Token: hacia; Lema: hacia\n",
      "Token: la; Lema: la\n",
      "Token: solución; Lema: solución\n",
      "Token: se; Lema: se\n",
      "Token: puede; Lema: puede\n",
      "Token: perder; Lema: perder\n",
      "Token: algo; Lema: algo\n",
      "Token: en; Lema: en\n",
      "Token: cada; Lema: cada\n",
      "Token: transición; Lema: transición\n",
      "Token: del; Lema: del\n",
      "Token: proceso; Lema: proceso\n",
      "Token: .; Lema: .\n",
      "Sentencia 8:\n",
      "Token: 7; Lema: 7\n",
      "Token: en; Lema: en\n",
      "Token: :; Lema: :\n",
      "Token: Drew; Lema: Drew\n",
      "Token: Conway; Lema: Conway\n",
      "Token: en; Lema: en\n",
      "Token: su; Lema: su\n",
      "Token: página; Lema: página\n",
      "Token: web; Lema: web\n",
      "Token: explica; Lema: explica\n",
      "Token: con; Lema: con\n",
      "Token: la; Lema: la\n",
      "Token: ayuda; Lema: ayuda\n",
      "Token: de; Lema: de\n",
      "Token: un; Lema: un\n",
      "Token: diagrama; Lema: diagrama\n",
      "Token: de; Lema: de\n",
      "Token: Venn; Lema: Venn\n",
      "Token: ,; Lema: ,\n",
      "Token: las; Lema: la\n",
      "Token: principales; Lema: principales\n",
      "Token: habilidades; Lema: habilidades\n",
      "Token: que; Lema: que\n",
      "Token: le; Lema: le\n",
      "Token: dan; Lema: dan\n",
      "Token: vida; Lema: vida\n",
      "Token: y; Lema: y\n",
      "Token: forma; Lema: forma\n",
      "Token: a; Lema: a\n",
      "Token: la; Lema: la\n",
      "Token: ciencia; Lema: ciencia\n",
      "Token: de; Lema: de\n",
      "Token: datos; Lema: datos\n",
      "Token: ,; Lema: ,\n",
      "Token: así; Lema: así\n",
      "Token: como; Lema: como\n",
      "Token: sus; Lema: sus\n",
      "Token: relaciones; Lema: relaciones\n",
      "Token: de; Lema: de\n",
      "Token: conjuntos; Lema: conjuntos\n",
      "Token: .; Lema: .\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Ciencia_de_datos.txt\", \"r\") as text:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(text.read())\n",
    "\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    word_tokenized.append(word_tokenize(s))\n",
    "\n",
    "for s_idx, s in enumerate(word_tokenized):\n",
    "    print(f\"Sentencia {s_idx}:\")\n",
    "    for w in s:\n",
    "        print(f\"Token: {w}; Lema: {WordNetLemmatizer().lemmatize(w)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
