{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 1:** dada la frase “Juan usa la bicicleta de Clara todos los días soleados.”, se pide realizar un análisis sintáctico parcial de forma que se obtengan los chunks que se muestran a continuación. Será necesario, por tanto, definir una gramática con las reglas que puedan identificar dichos chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Juan', 'NNP'), ('usa', 'JJ'), ('la', 'NN'), ('bicicleta', 'NN'), ('de', 'IN'), ('Clara', 'NNP'), ('todos', 'CC'), ('los', 'JJ'), ('días', 'JJ'), ('soleados', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (NP Juan/NNP usa/JJ la/NN bicicleta/NN)\n",
      "  (PP de/IN (NP Clara/NNP))\n",
      "  todos/CC\n",
      "  (NP los/JJ días/JJ soleados/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Juan usa la bicicleta de Clara todos los días soleados.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)\n",
    "grammar = r\"\"\"\n",
    "NP: {<NNP>?<JJ>*<NN>*}\n",
    "PP: {<IN><NP>?}\n",
    "\"\"\"\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 2:** a partir del contenido del fichero de texto “Cycling.txt”, se quiere hacer chunking\n",
    "para detectar diferentes sintagmas. Utilizar la clase RegexpParser de NLTK para\n",
    "crear gramáticas que contengan los patrones que se quieren identificar. Se quieren\n",
    "identificar los siguientes chunks (ponerlo todo en la misma gramática):\n",
    "- Chunks formados por 2 nombres (`A`).\n",
    "- Chunks formados por un determinante y un nombre (`B`).\n",
    "- Chunks formados por una preposición, puede que un determinante y un nombre (`C`).\n",
    "- Chunks formados por un verbo, un adjetivo y un nombre (`D`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('Cycling', 'VBG'), (\"'s\", 'POS'), Tree('2N', [('world', 'NN'), ('governing', 'NN')]), ('body', 'NN'), (',', ','), Tree('DETN', [('the', 'DT'), ('UCI', 'NNP')]), (',', ','), ('says', 'VBZ'), ('it', 'PRP'), ('has', 'VBZ'), ('no', 'DT'), ('plans', 'NNS'), ('to', 'TO'), ('move', 'VB'), ('the', 'DT'), ('2025', 'CD'), Tree('2N', [('Road', 'NNP'), ('World', 'NNP')]), ('Championships', 'NNP'), ('away', 'RB'), ('from', 'IN'), ('Rwanda', 'NNP'), ('amid', 'IN'), ('the', 'DT'), ('ongoing', 'JJ'), ('conflict', 'NN'), ('in', 'IN'), ('neighbouring', 'VBG'), Tree('2N', [('DR', 'NNP'), ('Congo', 'NNP')]), ('.', '.')]), Tree('S', [('Rwanda', 'NNP'), ('is', 'VBZ'), ('set', 'VBN'), ('to', 'TO'), ('become', 'VB'), ('the', 'DT'), ('first', 'JJ'), ('African', 'JJ'), ('nation', 'NN'), ('to', 'TO'), ('host', 'VB'), Tree('DETN', [('the', 'DT'), ('event', 'NN')]), ('from', 'IN'), ('21-28', 'JJ'), ('September', 'NNP'), ('.', '.')]), Tree('S', [('The', 'DT'), Tree('2N', [('M23', 'NNP'), ('rebel', 'NN')]), ('group', 'NN'), ('has', 'VBZ'), ('captured', 'VBN'), ('almost', 'RB'), ('all', 'DT'), ('of', 'IN'), ('the', 'DT'), ('eastern', 'JJ'), Tree('2N', [('Congolese', 'NNP'), ('city', 'NN')]), ('of', 'IN'), ('Goma', 'NNP'), ('and', 'CC'), ('threatened', 'VBD'), ('to', 'TO'), ('continue', 'VB'), ('its', 'PRP$'), ('offensive', 'JJ'), ('to', 'TO'), Tree('DETN', [('the', 'DT'), ('capital', 'NN')]), (',', ','), ('Kinshasa', 'NNP'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('2,600km', 'CD'), ('(', '('), ('1,600', 'CD'), ('miles', 'NNS'), (')', ')'), ('away', 'RB'), ('.', '.')]), Tree('S', [Tree('DETN', [('The', 'DT'), ('UCI', 'NNP')]), ('said', 'VBD'), ('it', 'PRP'), ('is', 'VBZ'), ('``', '``'), ('closely', 'RB'), ('monitoring', 'JJ'), ('developments', 'NNS'), (\"''\", \"''\"), ('and', 'CC'), ('their', 'PRP$'), ('potential', 'JJ'), ('impact', 'NN'), ('on', 'IN'), ('organisation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('championships', 'NNS'), ('in', 'IN'), ('Kigali', 'NNP'), ('.', '.')]), Tree('S', [('It', 'PRP'), ('added', 'VBD'), ('that', 'IN'), ('Rwanda', 'NNP'), ('``', '``'), ('remains', 'VBZ'), ('entirely', 'RB'), ('safe', 'JJ'), ('for', 'IN'), ('tourism', 'NN'), ('and', 'CC'), ('business', 'NN'), (\"''\", \"''\"), ('because', 'IN'), ('fighting', 'VBG'), ('is', 'VBZ'), ('confined', 'VBN'), ('to', 'TO'), Tree('2N', [('DR', 'NNP'), ('Congo', 'NNP')]), ('.', '.')]), Tree('S', [('``', '``'), ('Following', 'VBG'), Tree('DETN', [('the', 'DT'), ('spread', 'NN')]), ('of', 'IN'), ('rumours', 'NN'), ('on', 'IN'), Tree('DETN', [('this', 'DT'), ('subject', 'NN')]), (',', ','), Tree('DETN', [('the', 'DT'), ('UCI', 'NNP')]), ('clarifies', 'NNS'), ('that', 'IN'), Tree('DETN', [('no', 'DT'), ('relocation', 'NN')]), ('of', 'IN'), ('the', 'DT'), ('2025', 'CD'), Tree('2N', [('UCI', 'NNP'), ('Road', 'NNP')]), Tree('2N', [('World', 'NNP'), ('Championships', 'NNP')]), ('from', 'IN'), ('Rwanda', 'NNP'), ('to', 'TO'), ('Switzerland', 'NNP'), ('or', 'CC'), ('any', 'DT'), ('other', 'JJ'), ('location', 'NN'), ('is', 'VBZ'), ('planned', 'VBN'), ('at', 'IN'), Tree('DETN', [('this', 'DT'), ('time', 'NN')]), (',', ','), (\"''\", \"''\"), Tree('DETN', [('a', 'DT'), ('statement', 'NN')]), ('on', 'IN'), ('the', 'DT'), ('governing', 'VBG'), ('body', 'NN'), (\"'s\", 'POS'), ('website', 'NN'), ('said', 'VBD'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Cycling.txt\", \"r\") as f:\n",
    "    sent_tokenized = PunktSentenceTokenizer().tokenize(f.read())\n",
    "\n",
    "tagged_sents = []\n",
    "word_tokenized = []\n",
    "for s in sent_tokenized:\n",
    "    tokenized = word_tokenize(s)\n",
    "    word_tokenized.append(tokenized)\n",
    "    tagged_sents.append(nltk.pos_tag(tokenized))\n",
    "\n",
    "grammar = r\"\"\"\n",
    "A: {(<NNP>)}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "result = []\n",
    "for s in tagged_sents:\n",
    "    result.append(cp.parse(s))\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
