{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 1:** se pide hacer tokenización con ER. Dado un texto, hay que obtener todos los tokens dividiendo el texto separando por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'AEMET', 'lanza', 'una', 'alerta', 'por', 'nieve', 'y', 'una', 'nueva', 'ola', 'de', 'frío,', 'será', 'mejor', 'que', 'nos', 'preparemos', 'para', 'lo', 'peor', 'en', 'cuestión', 'de', 'horas.', 'Habrá', 'llegado', 'el', 'momento', 'de', 'empezar', 'a', 'pensar', 'en', 'lo', 'que', 'está', 'por', 'llegar,', 'por', 'lo', 'que,', 'habrá', 'llegado', 'el', 'momento', 'de', 'prepararnos', 'para', 'los', 'últimos', 'coletazos', 'del', 'invierno', 'y', 'hacerlo', 'de', 'tal', 'manera', 'que', 'tendremos', 'que', 'afrontar', 'una', 'recta', 'final', 'de', 'la', 'semana', 'cargada', 'de', 'acción.', 'Tenemos', 'que', 'empezar', 'a', 'pensar', 'en', 'lo', 'que', 'llegue', 'a', 'toda', 'velocidad.', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"La AEMET lanza una alerta por nieve y una nueva ola de frío, será\n",
    "mejor que nos preparemos para lo peor en cuestión de horas. Habrá llegado el\n",
    "momento de empezar a pensar en lo que está por llegar, por lo que, habrá llegado el\n",
    "momento de prepararnos para los últimos coletazos del invierno y hacerlo de tal\n",
    "manera que tendremos que afrontar una recta final de la semana cargada de acción.\n",
    "Tenemos que empezar a pensar en lo que llegue a toda velocidad.\n",
    "\"\"\"\n",
    "\n",
    "tokenized = re.split(r\"\\s\", text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 2:** se pide normalizar un texto a partir del uso de ER. Dado el siguiente texto, hay que normalizarlo haciendo varias tareas:\n",
    "    - Eliminar signos de puntuación.\n",
    "    - Eliminar palabras vacías del texto.\n",
    "    - Eliminar todas las palabras con más de 5 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Francia aspira a jugar un papel protagonista en el auge de los\n",
    "algoritmos. Esta semana, París fue el epicentro de una cumbre mundial sobre IA,\n",
    "donde expertos de todo el mundo se adentraron en las amenazas y promesas de esta\n",
    "tecnología. En el marco de este evento, el presidente Emmanuel Macron y el jeque\n",
    "Mohamed bin Zayed Al Nahyan, líder de los Emiratos Árabes Unidos, presenciaron la\n",
    "firma de un acuerdo de cooperación entre sus países, un pacto que promete\n",
    "potenciar el desarrollo de proyectos conjuntos.\n",
    "Como recoge la Agencia de Noticias de los Emiratos, la alianza incluye una inversión\n",
    "por parte de la nación rica en petróleo en Francia, así como “la adquisición de chips\n",
    "de vanguardia, la infraestructura de centros de datos y el desarrollo de talento, y\n",
    "mediante el establecimiento de Embajadas de Datos Virtuales para permitir la IA\n",
    "soberana y la infraestructura en la nube en ambos países”. El Gobierno francés, por\n",
    "su parte, ha señalado que la iniciativa contempla la construcción de un enorme\n",
    "centro de datos.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized = PunktSentenceTokenizer().tokenize(text)\n",
    "word_tokenized = [word_tokenize(s) for s in sent_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJ. 2, a) Eliminar signos de puntuación\n",
    "no_punct = []\n",
    "for s in word_tokenized:\n",
    "    no_punct.append([w for w in s if not re.match(r\"[.:,;]\", w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Francia', 'aspira', 'jugar', 'papel', 'protagonista', 'auge', 'algoritmos'], ['Esta', 'semana', 'París', 'epicentro', 'cumbre', 'mundial', 'IA', 'expertos', 'mundo', 'adentraron', 'amenazas', 'promesas', 'tecnología'], ['En', 'marco', 'evento', 'presidente', 'Emmanuel', 'Macron', 'jeque', 'Mohamed', 'bin', 'Zayed', 'Al', 'Nahyan', 'líder', 'Emiratos', 'Árabes', 'Unidos', 'presenciaron', 'firma', 'acuerdo', 'cooperación', 'países', 'pacto', 'promete', 'potenciar', 'desarrollo', 'proyectos', 'conjuntos'], ['Como', 'recoge', 'Agencia', 'Noticias', 'Emiratos', 'alianza', 'incluye', 'inversión', 'parte', 'nación', 'rica', 'petróleo', 'Francia', 'así', '“', 'adquisición', 'chips', 'vanguardia', 'infraestructura', 'centros', 'datos', 'desarrollo', 'talento', 'mediante', 'establecimiento', 'Embajadas', 'Datos', 'Virtuales', 'permitir', 'IA', 'soberana', 'infraestructura', 'nube', 'ambos', 'países', '”'], ['El', 'Gobierno', 'francés', 'parte', 'señalado', 'iniciativa', 'contempla', 'construcción', 'enorme', 'centro', 'datos']]\n"
     ]
    }
   ],
   "source": [
    "# EJ. 2, b) Eliminar palabras vacías\n",
    "spanish_sw = set(stopwords.words(\"spanish\"))\n",
    "no_sw = []\n",
    "for s in no_punct:\n",
    "    no_sw.append([w for w in s if w not in spanish_sw])\n",
    "print(no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jugar', 'papel', 'auge'],\n",
       " ['Esta', 'París', 'IA', 'mundo'],\n",
       " ['En', 'marco', 'jeque', 'bin', 'Zayed', 'Al', 'líder', 'firma', 'pacto'],\n",
       " ['Como',\n",
       "  'parte',\n",
       "  'rica',\n",
       "  'así',\n",
       "  '“',\n",
       "  'chips',\n",
       "  'datos',\n",
       "  'Datos',\n",
       "  'IA',\n",
       "  'nube',\n",
       "  'ambos',\n",
       "  '”'],\n",
       " ['El', 'parte', 'datos']]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EJ. 2, c) Eliminar palabras de longitud mayor a 5\n",
    "lt5 = []\n",
    "for s in no_sw:\n",
    "    lt5.append([w for w in s if not re.match(r\"\\b\\w{6,}\\b\", w)])\n",
    "lt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 3:** Dado el texto “Este es un texto que tiene ejemplos de fechas. Hoy es 09/02/2025, esta es una fecha posterior al 13 de enero de 2025. ¿Nos gustaría estar ya a 5 de julio y empezar las vacaciones? Casi que mejor no, que el tiempo avance a su ritmo. El primer día de clase fue el 30-01-2025, y el último día será el 8 de mayo del 2025.” Hay que construir expresiones regulares para que sean capaces de reconocer los diferentes formatos de fechas que aparecen (DD/MM/AAAA, DD-MM-AAAA, DD de MM de AAAA y DD de MM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Este es un texto que tiene ejemplos de fechas. Hoy es 09/02/2025,\n",
    "esta es una fecha posterior al 13 de enero de 2025. ¿Nos gustaría estar ya a 5 de julio\n",
    "y empezar las vacaciones? Casi que mejor no, que el tiempo avance a su ritmo. El\n",
    "primer día de clase fue el 30-01-2025, y el último día será el 8 de mayo del 2025.\n",
    "\"\"\"\n",
    "sent_tokenized = PunktSentenceTokenizer().tokenize(text)\n",
    "word_tokenized = [word_tokenize(s) for s in sent_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09/02/2025', '13 de enero de 2025', '5 de julio', '30-01-2025', '8 de mayo del 2025']\n"
     ]
    }
   ],
   "source": [
    "gram = r\"(\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{1,2}-\\d{1,2}-\\d{2,4}|\\d{1,2} de \\w+ del \\d{2,4}|\\d{1,2} de \\w+ de \\d{2,4}|\\d{1,2} de \\w+)\"\n",
    "print(re.findall(gram, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 4:** Dado un texto que contenga números de teléfono de España, fijos (puede comenzar por el prefijo 8 o 9) o móviles (pueden comenzar con 6 o 7) y formados por 9 dígitos en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hola soy x y mi móvil es 678903423 y mi fijo es 826738495\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['678903423', '826738495']\n"
     ]
    }
   ],
   "source": [
    "gram = r\"[6-9]\\d{8}\"\n",
    "print(re.findall(gram, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 5:** se tiene que diseñar una expresión regular que haga NER para nombres de personas, es decir, que dado un texto sea capaz de reconocer nombres de personas en él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Un premio cantado, el de Eduard Fernández (mejor actor por\n",
    "Marco), y otro que fue una sorpresa, el de Carolina Yuste por La infiltrada, dejaron la\n",
    "puerta abierta a que pudiese suceder cualquier cosa en la recta final de la noche.\n",
    "Incluso lo mejor que podía pasar sucedió. Subió a recoger el Goya a la mejor dirección\n",
    "Pol Rodríguez, uno de los responsables junto a Isaki Lacuesta de la película mejor\n",
    "dirigida del año: Segundo premio.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un', 'Eduard', 'Fernández', 'Marco', 'Carolina', 'Yuste', 'La', 'Incluso', 'Subió', 'Goya', 'Pol', 'Rodríguez', 'Isaki', 'Lacuesta', 'Segundo']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"[A-Z]\\w+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que se obtienen todas las palabras que comienzan por una letra mayúscula, pero utilizando ER es imposible mejorar mucho más el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 6:** Mediante ER, reconocer los acrónimos presentes en un texto. Pueden ser acrónimos como URJC o acrónimos como U.S.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['URJC']\n",
      "['U.S.A.', 'A.E.M.E.T.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Soy x y soy estudiante en la URJC, y me gustaría hacer las prácticas en U.S.A. y A.E.M.E.T.\"\n",
    "print(re.findall(r\"[A-Z]{2,}\", text))\n",
    "print(re.findall(r\"(?:[A-Z]\\.){2,}\", text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
