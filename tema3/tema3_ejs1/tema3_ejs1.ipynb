{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definimos el set de stopwords\n",
    "english_sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejercicio 1:** Utilizando una representación de bolsa de palabras (BoW) se quiere que los rasgos del\n",
    "vocabulario no sean palabras, sino n-gramas. Los n-gramas son secuencias contiguas de n elementos en un texto, cruciales para captar la información contextual. Pueden mejorar la representación de los textos al no considerar palabras sueltas, sino combinaciones de estas, lo que proporciona un contexto más rico.  \n",
    "Dadas las siguientes frases: “Estamos ya a finales de febrero.”, “En febrero sigue haciendo frío.”, “Esto es una frase de ejemplo que habla de un mes del año.” Se pide lo siguiente:\n",
    "- Generar una representación BoW con bigramas (2-grams).\n",
    "- Generar una representación BoW con unigramas, bigramas y trigramas.\n",
    "En cada caso se debe de imprimir el vocabulario generado y la matriz de rasgosdocumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repre. con BoW de bigramas\n",
      "Vocabulario:\n",
      "['de ejemplo' 'de febrero' 'de un' 'del año' 'ejemplo que' 'en febrero'\n",
      " 'es una' 'estamos ya' 'esto es' 'febrero sigue' 'finales de' 'frase de'\n",
      " 'habla de' 'haciendo frío' 'mes del' 'que habla' 'sigue haciendo'\n",
      " 'un mes' 'una frase' 'ya finales']\n",
      "Matriz rasgos-documentos:\n",
      "[[0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0]]\n",
      "\n",
      "\n",
      "Repre. con BoW de unigramas, bigramas y trigramas:\n",
      "Vocabulario:\n",
      "['año' 'de' 'de ejemplo' 'de ejemplo que' 'de febrero' 'de un' 'de un mes'\n",
      " 'del' 'del año' 'ejemplo' 'ejemplo que' 'ejemplo que habla' 'en'\n",
      " 'en febrero' 'en febrero sigue' 'es' 'es una' 'es una frase' 'estamos'\n",
      " 'estamos ya' 'estamos ya finales' 'esto' 'esto es' 'esto es una'\n",
      " 'febrero' 'febrero sigue' 'febrero sigue haciendo' 'finales' 'finales de'\n",
      " 'finales de febrero' 'frase' 'frase de' 'frase de ejemplo' 'frío' 'habla'\n",
      " 'habla de' 'habla de un' 'haciendo' 'haciendo frío' 'mes' 'mes del'\n",
      " 'mes del año' 'que' 'que habla' 'que habla de' 'sigue' 'sigue haciendo'\n",
      " 'sigue haciendo frío' 'un' 'un mes' 'un mes del' 'una' 'una frase'\n",
      " 'una frase de' 'ya' 'ya finales' 'ya finales de']\n",
      "Matriz rasgos-documentos:\n",
      "[[0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
      "  0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1\n",
      "  1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Estamos ya a finales de febrero.\",\n",
    "    \"En febrero sigue haciendo frío.\",\n",
    "    \"Esto es una frase de ejemplo que habla de un mes del año.\"\n",
    "]\n",
    "\n",
    "# Representación BoW con bigramas\n",
    "vectorizer1 = CountVectorizer(ngram_range=(2, 2))\n",
    "bow_encoded1 = vectorizer1.fit_transform(corpus)\n",
    "print(\"Repre. con BoW de bigramas\")\n",
    "print(\"Vocabulario:\")\n",
    "print(vectorizer1.get_feature_names_out())\n",
    "print(\"Matriz rasgos-documentos:\")\n",
    "print(bow_encoded1.toarray())\n",
    "\n",
    "# Representación BoW con unigramas, bigramas y trigramas\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 3))\n",
    "bow_encoded2 = vectorizer2.fit_transform(corpus)\n",
    "print(\"\\n\\nRepre. con BoW de unigramas, bigramas y trigramas:\")\n",
    "print(\"Vocabulario:\")\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(\"Matriz rasgos-documentos:\")\n",
    "print(bow_encoded2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 2:** Dadas varias frases, generar una representación BoW con función de pesado TF-IDF. Imprimir el vocabulario generado y la matriz de rasgos-documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:\n",
      "['año' 'de' 'del' 'ejemplo' 'en' 'es' 'estamos' 'esto' 'febrero' 'finales'\n",
      " 'frase' 'frío' 'habla' 'haciendo' 'mes' 'que' 'sigue' 'un' 'una' 'ya']\n",
      "Matriz rasgos-documentos:\n",
      "[[0.         0.37302199 0.         0.         0.         0.\n",
      "  0.49047908 0.         0.37302199 0.49047908 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.49047908]\n",
      " [0.         0.         0.         0.         0.46735098 0.\n",
      "  0.         0.         0.35543247 0.         0.         0.46735098\n",
      "  0.         0.46735098 0.         0.         0.46735098 0.\n",
      "  0.         0.        ]\n",
      " [0.27406418 0.41686575 0.27406418 0.27406418 0.         0.27406418\n",
      "  0.         0.27406418 0.         0.         0.27406418 0.\n",
      "  0.27406418 0.         0.27406418 0.27406418 0.         0.27406418\n",
      "  0.27406418 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos el corpus anterior\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow_encoded = vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulario:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Matriz rasgos-documentos:\")\n",
    "print(bow_encoded.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 3:** Dadas las siguientes frases:\n",
    "    - \"El saque de Nadal es imparable.\",\n",
    "    - \"Me encanta jugar al tenis los fines de semana.\",\n",
    "    - \"¿Viste el último partido de Federer?\",\n",
    "    - \"Necesito una nueva raqueta de tenis.\",\n",
    "    - \"El torneo de Wimbledon es mi favorito.\",\n",
    "    - \"Practicar el revés es fundamental para mejorar.\",\n",
    "    - \"El tenis es un deporte que requiere mucha concentración.\",\n",
    "    - \"¿Quién es tu tenista favorito?\",\n",
    "    - \"La pista de tierra batida es muy exigente.\",\n",
    "    - \"Vamos a jugar un partido de dobles.\"  \n",
    "\n",
    "Se pide lo siguiente:\n",
    "- Generar sus unigramas y mostrar el vocabulario con la frecuencia de cada uno en toda la colección.\n",
    "- Utilizar pandas para mostrar los unigramas por documento (por frase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrama: al; Frecuencia: 1\n",
      "Unigrama: batida; Frecuencia: 1\n",
      "Unigrama: concentración; Frecuencia: 1\n",
      "Unigrama: de; Frecuencia: 7\n",
      "Unigrama: deporte; Frecuencia: 1\n",
      "Unigrama: dobles; Frecuencia: 1\n",
      "Unigrama: el; Frecuencia: 5\n",
      "Unigrama: encanta; Frecuencia: 1\n",
      "Unigrama: es; Frecuencia: 6\n",
      "Unigrama: exigente; Frecuencia: 1\n",
      "Unigrama: favorito; Frecuencia: 2\n",
      "Unigrama: federer; Frecuencia: 1\n",
      "Unigrama: fines; Frecuencia: 1\n",
      "Unigrama: fundamental; Frecuencia: 1\n",
      "Unigrama: imparable; Frecuencia: 1\n",
      "Unigrama: jugar; Frecuencia: 2\n",
      "Unigrama: la; Frecuencia: 1\n",
      "Unigrama: los; Frecuencia: 1\n",
      "Unigrama: me; Frecuencia: 1\n",
      "Unigrama: mejorar; Frecuencia: 1\n",
      "Unigrama: mi; Frecuencia: 1\n",
      "Unigrama: mucha; Frecuencia: 1\n",
      "Unigrama: muy; Frecuencia: 1\n",
      "Unigrama: nadal; Frecuencia: 1\n",
      "Unigrama: necesito; Frecuencia: 1\n",
      "Unigrama: nueva; Frecuencia: 1\n",
      "Unigrama: para; Frecuencia: 1\n",
      "Unigrama: partido; Frecuencia: 2\n",
      "Unigrama: pista; Frecuencia: 1\n",
      "Unigrama: practicar; Frecuencia: 1\n",
      "Unigrama: que; Frecuencia: 1\n",
      "Unigrama: quién; Frecuencia: 1\n",
      "Unigrama: raqueta; Frecuencia: 1\n",
      "Unigrama: requiere; Frecuencia: 1\n",
      "Unigrama: revés; Frecuencia: 1\n",
      "Unigrama: saque; Frecuencia: 1\n",
      "Unigrama: semana; Frecuencia: 1\n",
      "Unigrama: tenis; Frecuencia: 3\n",
      "Unigrama: tenista; Frecuencia: 1\n",
      "Unigrama: tierra; Frecuencia: 1\n",
      "Unigrama: torneo; Frecuencia: 1\n",
      "Unigrama: tu; Frecuencia: 1\n",
      "Unigrama: un; Frecuencia: 2\n",
      "Unigrama: una; Frecuencia: 1\n",
      "Unigrama: vamos; Frecuencia: 1\n",
      "Unigrama: viste; Frecuencia: 1\n",
      "Unigrama: wimbledon; Frecuencia: 1\n",
      "Unigrama: último; Frecuencia: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>al</th>\n",
       "      <th>batida</th>\n",
       "      <th>concentración</th>\n",
       "      <th>de</th>\n",
       "      <th>deporte</th>\n",
       "      <th>dobles</th>\n",
       "      <th>el</th>\n",
       "      <th>encanta</th>\n",
       "      <th>es</th>\n",
       "      <th>exigente</th>\n",
       "      <th>...</th>\n",
       "      <th>tenista</th>\n",
       "      <th>tierra</th>\n",
       "      <th>torneo</th>\n",
       "      <th>tu</th>\n",
       "      <th>un</th>\n",
       "      <th>una</th>\n",
       "      <th>vamos</th>\n",
       "      <th>viste</th>\n",
       "      <th>wimbledon</th>\n",
       "      <th>último</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   al  batida  concentración  de  deporte  dobles  el  encanta  es  exigente  \\\n",
       "0   0       0              0   1        0       0   1        0   1         0   \n",
       "1   1       0              0   1        0       0   0        1   0         0   \n",
       "2   0       0              0   1        0       0   1        0   0         0   \n",
       "3   0       0              0   1        0       0   0        0   0         0   \n",
       "4   0       0              0   1        0       0   1        0   1         0   \n",
       "\n",
       "   ...  tenista  tierra  torneo  tu  un  una  vamos  viste  wimbledon  último  \n",
       "0  ...        0       0       0   0   0    0      0      0          0       0  \n",
       "1  ...        0       0       0   0   0    0      0      0          0       0  \n",
       "2  ...        0       0       0   0   0    0      0      1          0       1  \n",
       "3  ...        0       0       0   0   0    1      0      0          0       0  \n",
       "4  ...        0       0       1   0   0    0      0      0          1       0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"El saque de Nadal es imparable.\",\n",
    "    \"Me encanta jugar al tenis los fines de semana.\",\n",
    "    \"¿Viste el último partido de Federer?\",\n",
    "    \"Necesito una nueva raqueta de tenis.\",\n",
    "    \"El torneo de Wimbledon es mi favorito.\",\n",
    "    \"Practicar el revés es fundamental para mejorar.\",\n",
    "    \"El tenis es un deporte que requiere mucha concentración.\",\n",
    "    \"¿Quién es tu tenista favorito?\",\n",
    "    \"La pista de tierra batida es muy exigente.\",\n",
    "    \"Vamos a jugar un partido de dobles.\"  \n",
    "]\n",
    "\n",
    "# Mostramos el vocabulario con la frecuencia de cada unigrama\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded = vectorizer.fit_transform(corpus)\n",
    "\n",
    "for idx, unigram in enumerate(vectorizer.get_feature_names_out()):\n",
    "    print(f\"Unigrama: {unigram}; Frecuencia: {sum(bow_encoded.toarray()[:, idx])}\")\n",
    "\n",
    "# Mostramos los unigramas por documento\n",
    "df = pd.DataFrame(bow_encoded.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 4:** Dada una frase cualquiera, utilizando la librería NLTK, se pide los siguiente:\n",
    "    - Calcular y mostrar los bigramas\n",
    "    - Calcular y mostrar los trigramas.\n",
    "    - Haz lo mismo que en los dos casos anteriores, pero añadiendo relleno (inicio y fin de frase).  \n",
    "    \n",
    "El padding o relleno sirve para que cada componente del n-grama aparezca en todas las posiciones del n-grama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas: [('Hoy', 'es'), ('es', 'un'), ('un', 'día'), ('día', 'soleado'), ('soleado', 'por'), ('por', 'fin'), ('fin', '.')]\n",
      "Trigramas: [('Hoy', 'es', 'un'), ('es', 'un', 'día'), ('un', 'día', 'soleado'), ('día', 'soleado', 'por'), ('soleado', 'por', 'fin'), ('por', 'fin', '.')]\n",
      "Bigramas con relleno: [(None, 'Hoy'), ('Hoy', 'es'), ('es', 'un'), ('un', 'día'), ('día', 'soleado'), ('soleado', 'por'), ('por', 'fin'), ('fin', '.'), ('.', None)]\n",
      "Trigramas con relleno: [(None, None, 'Hoy'), (None, 'Hoy', 'es'), ('Hoy', 'es', 'un'), ('es', 'un', 'día'), ('un', 'día', 'soleado'), ('día', 'soleado', 'por'), ('soleado', 'por', 'fin'), ('por', 'fin', '.'), ('fin', '.', None), ('.', None, None)]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Hoy es un día soleado por fin.\"\n",
    "word_tokenized = word_tokenize(sent)\n",
    "\n",
    "# Calculamos y mostramos los bigramas\n",
    "bigrams = list(nltk.ngrams(word_tokenized, 2))\n",
    "print(f\"Bigramas: {bigrams}\")\n",
    "\n",
    "# Calculamos y mostramos los trigramas\n",
    "trigrams = list(nltk.ngrams(word_tokenized, 3))\n",
    "print(f\"Trigramas: {trigrams}\")\n",
    "\n",
    "# Añadimos relleno\n",
    "bigrams_pad = list(nltk.ngrams(word_tokenized, 2, pad_right=True, pad_left=True))\n",
    "print(f\"Bigramas con relleno: {bigrams_pad}\")\n",
    "trigrams_pad = list(nltk.ngrams(word_tokenized, 3, pad_right=True, pad_left=True))\n",
    "print(f\"Trigramas con relleno: {trigrams_pad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 5:** Dadas las frases del ejercicio 3, se pide, utilizando la librería sklearn, obtener la matriz de similitud coseno.\n",
    "    - Deberás utilizar la función de pesado binaria.\n",
    "    - Deberás utilizar la función de pesado TF.\n",
    "    - Deberás utilizar la función de pesado TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con fun. de pesado binaria:\n",
      "[[1.         0.13608276 0.33333333 0.16666667 0.46291005 0.3086067\n",
      "  0.27216553 0.18257419 0.28867513 0.16666667]\n",
      " [0.13608276 1.         0.13608276 0.27216553 0.12598816 0.\n",
      "  0.11111111 0.         0.11785113 0.27216553]\n",
      " [0.33333333 0.13608276 1.         0.16666667 0.3086067  0.15430335\n",
      "  0.13608276 0.         0.14433757 0.33333333]\n",
      " [0.16666667 0.27216553 0.16666667 1.         0.15430335 0.\n",
      "  0.13608276 0.         0.14433757 0.16666667]\n",
      " [0.46291005 0.12598816 0.3086067  0.15430335 1.         0.28571429\n",
      "  0.25197632 0.3380617  0.26726124 0.15430335]\n",
      " [0.3086067  0.         0.15430335 0.         0.28571429 1.\n",
      "  0.25197632 0.16903085 0.13363062 0.        ]\n",
      " [0.27216553 0.11111111 0.13608276 0.13608276 0.25197632 0.25197632\n",
      "  1.         0.1490712  0.11785113 0.13608276]\n",
      " [0.18257419 0.         0.         0.         0.3380617  0.16903085\n",
      "  0.1490712  1.         0.15811388 0.        ]\n",
      " [0.28867513 0.11785113 0.14433757 0.14433757 0.26726124 0.13363062\n",
      "  0.11785113 0.15811388 1.         0.14433757]\n",
      " [0.16666667 0.27216553 0.33333333 0.16666667 0.15430335 0.\n",
      "  0.13608276 0.         0.14433757 1.        ]]\n",
      "\n",
      "\n",
      "Con fun. de pesado TF:\n",
      "[[1.         0.13608276 0.33333333 0.16666667 0.46291005 0.3086067\n",
      "  0.27216553 0.18257419 0.28867513 0.16666667]\n",
      " [0.13608276 1.         0.13608276 0.27216553 0.12598816 0.\n",
      "  0.11111111 0.         0.11785113 0.27216553]\n",
      " [0.33333333 0.13608276 1.         0.16666667 0.3086067  0.15430335\n",
      "  0.13608276 0.         0.14433757 0.33333333]\n",
      " [0.16666667 0.27216553 0.16666667 1.         0.15430335 0.\n",
      "  0.13608276 0.         0.14433757 0.16666667]\n",
      " [0.46291005 0.12598816 0.3086067  0.15430335 1.         0.28571429\n",
      "  0.25197632 0.3380617  0.26726124 0.15430335]\n",
      " [0.3086067  0.         0.15430335 0.         0.28571429 1.\n",
      "  0.25197632 0.16903085 0.13363062 0.        ]\n",
      " [0.27216553 0.11111111 0.13608276 0.13608276 0.25197632 0.25197632\n",
      "  1.         0.1490712  0.11785113 0.13608276]\n",
      " [0.18257419 0.         0.         0.         0.3380617  0.16903085\n",
      "  0.1490712  1.         0.15811388 0.        ]\n",
      " [0.28867513 0.11785113 0.14433757 0.14433757 0.26726124 0.13363062\n",
      "  0.11785113 0.15811388 1.         0.14433757]\n",
      " [0.16666667 0.27216553 0.33333333 0.16666667 0.15430335 0.\n",
      "  0.13608276 0.         0.14433757 1.        ]]\n",
      "\n",
      "\n",
      "Con fun. de pesado TF-IDF:\n",
      "[[1.         0.04401798 0.14431656 0.05512477 0.20794414 0.13700281\n",
      "  0.12372398 0.07306769 0.10451453 0.05748398]\n",
      " [0.04401798 1.         0.04174201 0.13180107 0.04041363 0.\n",
      "  0.07673003 0.         0.03393445 0.16690679]\n",
      " [0.14431656 0.04174201 1.         0.05227451 0.13249941 0.07149173\n",
      "  0.06456248 0.         0.04478935 0.2202967 ]\n",
      " [0.05512477 0.13180107 0.05227451 1.         0.05061096 0.\n",
      "  0.09609085 0.         0.04249692 0.05172169]\n",
      " [0.20794414 0.04041363 0.13249941 0.05061096 1.         0.12578453\n",
      "  0.11359303 0.235307   0.09595652 0.05277699]\n",
      " [0.13700281 0.         0.07149173 0.         0.12578453 1.\n",
      "  0.10259138 0.06058741 0.04749885 0.        ]\n",
      " [0.12372398 0.07673003 0.06456248 0.09609085 0.11359303 0.10259138\n",
      "  1.         0.05471505 0.04289509 0.13091317]\n",
      " [0.07306769 0.         0.         0.         0.235307   0.06058741\n",
      "  0.05471505 1.         0.05632953 0.        ]\n",
      " [0.10451453 0.03393445 0.04478935 0.04249692 0.09595652 0.04749885\n",
      "  0.04289509 0.05632953 1.         0.0443157 ]\n",
      " [0.05748398 0.16690679 0.2202967  0.05172169 0.05277699 0.\n",
      "  0.13091317 0.         0.0443157  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Utilizando la función de pesado binaria\n",
    "vectorizer_binary = CountVectorizer(binary=True)\n",
    "bow_encoded_binary = vectorizer.fit_transform(corpus)\n",
    "cosine_sim_binary = cosine_similarity(bow_encoded_binary.toarray())\n",
    "\n",
    "# Utilizando la función de pesado TF\n",
    "vectorizer_tf = CountVectorizer()\n",
    "bow_encoded_tf = vectorizer_tf.fit_transform(corpus)\n",
    "cosine_sim_tf = cosine_similarity(bow_encoded_tf.toarray())\n",
    "\n",
    "# Utilizando la función de pesado TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "bow_encoded_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
    "cosine_sim_tfidf = cosine_similarity(bow_encoded_tfidf.toarray())\n",
    "\n",
    "print(\"Con fun. de pesado binaria:\")\n",
    "print(cosine_sim_binary)\n",
    "print(\"\\n\\nCon fun. de pesado TF:\")\n",
    "print(cosine_sim_tf)\n",
    "print(\"\\n\\nCon fun. de pesado TF-IDF:\")\n",
    "print(cosine_sim_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Ejercicio 6:** Utilizando el corpus movie_reviews de la librería NLTK, que es un corpus etiquetado con opiniones sobre películas, se pide utilizar el clasificador MultinomialNB para comprobar qué tipo de representación vectorial puede funcionar mejor para este problema. Hay que dividir el corpus en un 80% para entrenamiento y el 20% restante para test, de forma que para cada prueba que se haga hay que calcular el accuracy y ver cuál va mejor.  \n",
    "    - Representar con BoW y pesado binario.\n",
    "    - Representar con BoW y pesado TF.\n",
    "    - Representar con BoW y pesado TF-IDF.\n",
    "    - Lo mismo que en los casos anteriores, pero haciendo diferentes preprocesamientos:\n",
    "        - En el preprocesamiento de los textos, eliminar las stopwords. Probar a obtener de nuevo el accuracy con los tres pesados anteriores.\n",
    "        - Añadir a la eliminación de stopwords, la lematización, combinado con los tres pesados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tam. de X_train: (1600,)\n",
      "Tam. de X_test: (400,)\n"
     ]
    }
   ],
   "source": [
    "# Obtener datos\n",
    "data = []\n",
    "for categ in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(categ):\n",
    "        text = movie_reviews.open(fileid).read()\n",
    "        data.append((text, categ))\n",
    "\n",
    "df = pd.DataFrame(data, columns=(\"text\", \"categ\"))\n",
    "X = df[\"text\"]\n",
    "y = df[\"categ\"]\n",
    "\n",
    "# Dividir conjunto de datos en entrenamiento (80%) y test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(f\"Tam. de X_train: {X_train.shape}\")\n",
    "print(f\"Tam. de X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado binario\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train)\n",
    "bow_encoded_test = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado TF: 0.7925\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train)\n",
    "bow_encoded_test = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado TF: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario: 0.7975\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train)\n",
    "bow_encoded_test = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamiento del texto: eliminar las stopwords.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sw(text):\n",
    "    word_tokenized = regexp_tokenize(text, r\"[\\w|\\d]+\")\n",
    "    word_tokenized = [word for word in word_tokenized if word.lower() not in english_sw]\n",
    "    return \" \".join(word_tokenized)\n",
    "\n",
    "# Aplicamos el preprocesamiento al DF\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "X_train_preprocessed_sw = X_train_copy.apply(preprocess_sw)\n",
    "X_test_preprocessed_sw = X_test_copy.apply(preprocess_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario (sin stopwords): 0.8275\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado binario (Con eliminación de stopwords)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado TF (sin stopwords): 0.805\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF (Con eliminación de stopwords)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado TF (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario (sin stopwords): 0.8025\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF-IDF (Con eliminación de stopwords)\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamiento de texto: eliminación de stopwords + lematización.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para obtener los tags de WordNet a partir de los de NLTK\n",
    "def get_wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    elif nltk_tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Preprocesar texto con eliminación de stopwords y lematización\n",
    "def preprocess_sw_lemmatize(text):\n",
    "    word_tokenized = regexp_tokenize(text, r\"\\w+\")\n",
    "    word_tokenized = [word for word in word_tokenized if word.lower() not in english_sw]\n",
    "    tagged_tokens = nltk.pos_tag(word_tokenized)\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, get_wn_tag(tag)) for word, tag in tagged_tokens if get_wn_tag(tag) is not None]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Aplicamos el preprocesamiento al DF\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "X_train_preprocessed_sw_lemmatized = X_train_copy.apply(preprocess_sw_lemmatize)\n",
    "X_test_preprocessed_sw_lemmatized = X_test_copy.apply(preprocess_sw_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario (sin stopwords): 0.8125\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado binario (Con eliminación de stopwords y lematización)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw_lemmatized)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw_lemmatized)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado TF (sin stopwords): 0.7925\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF (Con eliminación de stopwords y lematización)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw_lemmatized)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw_lemmatized)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado TF (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con repre. con BoW y pesado binario (sin stopwords): 0.7875\n"
     ]
    }
   ],
   "source": [
    "# Repre. con BoW y pesado TF-IDF (Con eliminación de stopwords y lematización)\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow_encoded_train = vectorizer.fit_transform(X_train_preprocessed_sw_lemmatized)\n",
    "bow_encoded_test = vectorizer.transform(X_test_preprocessed_sw_lemmatized)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow_encoded_train.toarray(), y_train)\n",
    "y_pred = classifier.predict(bow_encoded_test)\n",
    "print(f\"Accuracy con repre. con BoW y pesado binario (sin stopwords): {accuracy_score(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
